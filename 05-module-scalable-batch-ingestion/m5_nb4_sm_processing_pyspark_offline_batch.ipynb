{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Offline Batch ingestion via SageMaker Processing job using Feature Store Spark Connector\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "Please be aware, that you need to run through the notebook [m5_nb0_partition_data](https://github.com/aws-samples/amazon-sagemaker-feature-store-end-to-end-workshop/blob/main/05-module-scalable-batch-ingestion/m5_nb0_partition_data.ipynb) in this section of the workshop, to setup the needed data. \n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Extending a PySpark Container and pushing to ECR](#ExtendPySparkContainer)\n",
    "1. [Create PySpark SageMaker Processing script](#Create-PySpark-SageMaker-Processing-script)\n",
    "1. [Run batch ingestion job](#Run-batch-ingestion-job)\n",
    "1. [Verify processing job results](#Verify-processing-job-results)\n",
    "\n",
    "\n",
    "\n",
    "In this example, an alteranative route through the batch ingestion via PySpark Processing containers will be explored to ingest data direclty into the Offline Store. We will use the `.ingest_data()` api instead of the `.put_record()` api. \n",
    "This circumvents using the Online Store and saves cost when only the offline store is needed. \n",
    "\n",
    "\n",
    "To achieve this, the package [Sagemaker Feature Store Pyspark](https://pypi.org/project/sagemaker-feature-store-pyspark/) is needed. If you want to use other means of Spark, please see the [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html) for further guidance.\n",
    "The usage with a PySpark Processing container, requires us to extend the PySpark Processing docker image, push it to ECR to have all the needed packages ready. \n",
    "\n",
    "If you want to execute the docker builds from SageMaker Studio, please set it up as instructed in this [blog](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/). Otherwise execute all command blocks that are marked with `docker needed` in an environment with accesss to the internet and docker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from random import randint\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = get_execution_role()\n",
    "except:\n",
    "    # for local dev, please set your sagemaker role here\n",
    "    role = 'arn:aws:iam::XXXXXXXX:role/service-role/role-name'\n",
    "logger.info(f'Role = {role}')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "featurestore_runtime_client = sagemaker_session.boto_session.client('sagemaker-featurestore-runtime', region_name=region)\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "logger.info(f'Default bucket = {default_bucket}')\n",
    "prefix = 'sagemaker-feature-store'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending a PySpark Container and pushing to ECR\n",
    "\n",
    "To work with the feature store manager as shown in the documentation, the PySpark Processing container needs to be extended. \n",
    "\n",
    "Therefore, access to docker is required. In the following section, we will use the plain docker commands, without utilizing the the possibilities of using sagemaker studio docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a folder for our docker files\n",
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "# Docker image of the current PySpark processing container with spark 3.2, python 3.9 and optimized for CPU\n",
    "FROM 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.2-cpu-py39-v1.0\n",
    "\n",
    "# Upgrade pip \n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "# Install the sagemaker feature store pyspark connector \n",
    "RUN pip3 install sagemaker-feature-store-pyspark-3.2 --no-binary :all: --verbose\n",
    "\n",
    "# Send logs direct to the terminal\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "# Set the entrypoint \n",
    "ENTRYPOINT [ \"smspark-submit\", \"processing.py\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "tag = ':latest'\n",
    "ecr_repository = 'pyspark-feature-store-spark-batch-ingestion'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "region, ecr_repository, processing_repository_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the docker file with regular docker\n",
    "\n",
    "The following commands will run only with docker running on you current machine. \n",
    "If you run on SageMaker Studio, please use the sm-docker library. This [blog](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/) descirbes how to setup sagemaker docker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker needed\n",
    "\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com \n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 173754725891.dkr.ecr.{region}.amazonaws.com \n",
    "!docker build -t $ecr_repository . --file docker/Dockerfile\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "making sure that everything is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr_client = boto3.client('ecr')\n",
    "ecr_client.describe_images(repositoryName=ecr_repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PySpark SageMaker Processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/batch_ingest_sm_pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/batch_ingest_sm_pyspark.py\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
    "from feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n",
    "from pyspark.sql.functions import udf, datediff, to_date, lit, col,isnan, when, count\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from argparse import Namespace, ArgumentParser\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "\n",
    "def transform_row(row) -> list:\n",
    "    columns = list(row.asDict())\n",
    "    record = []\n",
    "    for column in columns:\n",
    "        feature = {'FeatureName': column, 'ValueAsString': str(row[column])}\n",
    "        record.append(feature)\n",
    "    return record\n",
    "\n",
    "def ingest_to_feature_store(args: argparse.Namespace, rows) -> None:\n",
    "    feature_group_name = args.feature_group_name\n",
    "    session = boto3.session.Session()\n",
    "    featurestore_runtime_client = session.client(service_name='sagemaker-featurestore-runtime')\n",
    "    rows = list(rows)\n",
    "    logger.info(f'Ingesting {len(rows)} rows into feature group: {feature_group_name}')\n",
    "    for _, row in enumerate(rows):\n",
    "        record = transform_row(row)\n",
    "        response = featurestore_runtime_client.put_record(FeatureGroupName=feature_group_name, Record=record)\n",
    "        assert response['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "\n",
    "def batch_ingest_to_feature_store(args: argparse.Namespace, df: DataFrame) -> None:\n",
    "    feature_group_name = args.feature_group_name\n",
    "    logger.info(f'Feature Group name supplied is: {feature_group_name}')\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    logger.info(f'Instantiating FeatureStoreManger!')\n",
    "    feature_store_manager=FeatureStoreManager()\n",
    "\n",
    "    logger.info(f'trying to load datatypes directly from Dataframe')\n",
    "\n",
    "    # Load the feature definitions from input schema. The feature definitions can be used to create a feature group\n",
    "    feature_definitions = feature_store_manager.load_feature_definitions_from_schema(df)\n",
    "    logger.info(f'Feature definitions loaded successfully!')\n",
    "    print(feature_definitions)\n",
    "    feature_group_arn = args.feature_group_arn\n",
    "    logger.info(f'Feature Group ARN supplied is: {feature_group_arn}')\n",
    "\n",
    "    # If only OfflineStore is selected, the connector will batch write the data to offline store directly\n",
    "    args.target_feature_store_list = ast.literal_eval(args.target_feature_store_list)\n",
    "    logger.info(f'Ingesting into the following stores: {args.target_feature_store_list}')\n",
    "\n",
    "    feature_store_manager.ingest_data(input_data_frame=df, feature_group_arn=feature_group_arn, target_stores= args.target_feature_store_list) \n",
    "    logger.info(f'Feature Ingestions successful!')\n",
    "\n",
    "\n",
    "def parse_args() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_processes', type=int, default=1)\n",
    "    parser.add_argument('--num_workers', type=int, default=1)\n",
    "    parser.add_argument('--feature_group_name', type=str)\n",
    "    parser.add_argument('--feature_group_arn', type=str)\n",
    "    parser.add_argument('--target_feature_store_list', type=str)\n",
    "    parser.add_argument('--s3_uri_prefix', type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def check_data_quality(df: DataFrame) -> DataFrame:\n",
    "    # Sanity checking secktion \n",
    "    logger.info(f'First 5 rows of the dataframe for inspection: {df.show(5)}')\n",
    "\n",
    "    df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "    ).show()\n",
    "\n",
    "    # checking for categorical columns\n",
    "    categorical_cols = [field for (field, dataType) in df.dtypes if dataType == 'string']\n",
    "    logger.info(f'Categorical columns: {categorical_cols}')\n",
    "\n",
    "    # checking for numerical columns\n",
    "    numerical_cols = [field for (field, dataType) in df.dtypes if ((dataType == 'double') | (dataType == 'int') | (dataType == 'float'))]\n",
    "    logger.info(f'Numerical columns: {numerical_cols}')\n",
    "\n",
    "    # checking for boolean columns  \n",
    "    boolean_cols = [field for (field, dataType) in df.dtypes if dataType == 'boolean']\n",
    "    logger.info(f'Boolean columns: {boolean_cols}')\n",
    "\n",
    "    # checking for date columns\n",
    "    date_cols = [field for (field, dataType) in df.dtypes if dataType == 'date']\n",
    "    logger.info(f'Date columns: {date_cols}')\n",
    "\n",
    "\n",
    "def scale_col(df: DataFrame, col_name: str) -> DataFrame:\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]), 2), DoubleType())\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=f'{col_name}_vec')\n",
    "    # scale an column col_name with minmax scaler and drop the original column\n",
    "\n",
    "    scaler = MinMaxScaler(inputCol=f'{col_name}_vec', outputCol=f'{col_name}_scaled')\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    df = pipeline.fit(df).transform(df).withColumn(f'{col_name}_scaled', unlist(f'{col_name}_scaled')) \\\n",
    "                                       .drop(f'{col_name}_vec')\n",
    "    df = df.drop(col_name)\n",
    "    df = df.withColumnRenamed(f'{col_name}_scaled', col_name)\n",
    "    return df\n",
    "\n",
    "def ordinal_encode_col(df: DataFrame, col_name: str) -> DataFrame:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f'{col_name}_new')\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    df = df.drop(col_name)\n",
    "    df = df.withColumnRenamed(f'{col_name}_new', col_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_spark_job():\n",
    "\n",
    "    args = parse_args()\n",
    "    #add further packages as needed\n",
    "    pkg_list = []\n",
    "    pkg_list.append(\"software.amazon.sagemaker.featurestore:sagemaker-feature-store-spark-sdk_2.12:1.1.0\")\n",
    "    packages=(\",\".join(pkg_list))\n",
    "\n",
    "    logger.info(f'Added the following packages to Spark: {packages}')\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkJobFeatureStore\") \\\n",
    "        .config(\"spark.jars.packages\", packages) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # set the legacy time parser policy to LEGACY to allow for parsing of dates in the format dd/MM/yyyy HH:mm:ss, which solves backwards compatibility issues to spark 2.4\n",
    "    spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "    logger.info(f'Using Spark-Version:{spark.version}')\n",
    "\n",
    "    # get the total number of cores in the Spark cluster; if developing locally, there might be no executor\n",
    "    try:\n",
    "        spark_context = spark.sparkContext\n",
    "        total_cores = int(spark_context._conf.get('spark.executor.instances')) * int(spark_context._conf.get('spark.executor.cores'))\n",
    "        logger.info(f'Total available cores in the Spark cluster = {total_cores}')\n",
    "    except:\n",
    "        total_cores = 1\n",
    "        logger.info('Could not retrieve number of total cores. Setting total cores to 1')\n",
    "    \n",
    "    logger.info(f'Reading input file from S3. S3 uri is {args.s3_uri_prefix}')\n",
    "\n",
    "    # define the schema of the input data\n",
    "    csvSchema = StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"purchase_amount\", FloatType(), False),\n",
    "        StructField(\"is_reordered\", IntegerType(), False),\n",
    "        StructField(\"purchased_on\", StringType(), False),\n",
    "        StructField(\"event_time\", StringType(), False)])\n",
    "\n",
    "\n",
    "    # read the pyspark dataframe with a schema \n",
    "    df = spark.read.option(\"header\", \"true\").schema(csvSchema).csv(args.s3_uri_prefix)  \n",
    "    \n",
    "    # check the data quality of the dataframe and write findings to logs for inspection \n",
    "    check_data_quality(df)\n",
    "\n",
    "    # transform 1 - encode boolean to int\n",
    "    df = ordinal_encode_col(df, 'is_reordered')\n",
    "    df = df.withColumn('is_reordered', df['is_reordered'].cast(IntegerType()))\n",
    "\n",
    "    # transform 2 - min max scale `purchase_amount`\n",
    "    df = df.withColumn('purchase_amount', df['purchase_amount'].cast(DoubleType()))\n",
    "    df = scale_col(df, 'purchase_amount')\n",
    "    \n",
    "    # transform 3 - derive `n_days_since_last_purchase` column using the `purchased_on` col\n",
    "    current_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    df = df.withColumn('n_days_since_last_purchase', datediff(to_date(lit(current_date)), to_date('purchased_on', 'yyyy-MM-dd')))\n",
    "    df = df.drop('purchased_on')\n",
    "    df = scale_col(df, 'n_days_since_last_purchase')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Number of partitions = {df.rdd.getNumPartitions()}')\n",
    "    # Rule of thumb heuristic - rely on the product of #executors by #executor.cores, and then multiply that by 3 or 4\n",
    "    df = df.repartition(total_cores * 3)\n",
    "    logger.info(f'Number of partitions after re-partitioning = {df.rdd.getNumPartitions()}')\n",
    "    logger.info(f'Feature Store ingestion start: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n",
    "    batch_ingest_to_feature_store(args, df)\n",
    "    logger.info(f'Feature Store ingestion complete: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('BATCH INGESTION - STARTED')\n",
    "    run_spark_job()\n",
    "    logger.info('BATCH INGESTION - COMPLETED')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir jars\n",
    "# use wget to download the jar and place it in the jars folder, please check for the right version of the jar at time \n",
    "!wget -P jars https://repo1.maven.org/maven2/software/amazon/sagemaker/featurestore/sagemaker-feature-store-spark-sdk_2.12/1.1.0/sagemaker-feature-store-spark-sdk_2.12-1.1.0.jar\n",
    "# rename the jar to a shorter name for convenience which is sagemaker-feature-store-spark-sdk.jar\n",
    "!mv jars/sagemaker-feature-store-spark-sdk_2.12-1.1.0.jar jars/sagemaker-feature-store-spark-sdk.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run batch ingestion job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r orders_feature_group_name\n",
    "s3_uri_prefix = f's3://{default_bucket}/{prefix}/partitions/*'\n",
    "# REUSE orders feature group name from module 1\n",
    "feature_group_name = orders_feature_group_name \n",
    "feature_group_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client=boto3.client('sagemaker')\n",
    "\n",
    "feature_group_description = sm_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "feature_group_arn = feature_group_description['FeatureGroupArn']\n",
    "\n",
    "# please specify what target stores you want to ingest into -> PySpark does not accept list as a parameter\n",
    "target_feature_store_list = \"['OfflineStore']\" # ['OfflineStore', 'OnlineStore'] for both\n",
    "\n",
    "feature_group_name, feature_group_arn, target_feature_store_list, s3_uri_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_description[\"FeatureDefinitions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-preprocessor\",\n",
    "    image_uri = processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "pyspark_processor.run(submit_app='./scripts/batch_ingest_sm_pyspark.py', \n",
    "                      arguments = ['--feature_group_name', feature_group_name, \n",
    "                                   '--s3_uri_prefix', s3_uri_prefix,\n",
    "                                   '--feature_group_arn', feature_group_arn,\n",
    "                                   '--target_feature_store_list', target_feature_store_list],\n",
    "                      submit_jars=[\"jars/sagemaker-feature-store-spark-sdk.jar\"],\n",
    "                      spark_event_logs_s3_uri=f's3://{default_bucket}/spark-logs', \n",
    "                      logs=False,\n",
    "                      wait=True)  # set logs=True to disable logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processing job results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_id =  f'O{randint(1, 100000)}'\n",
    "logger.info(f'order_id={order_id}') \n",
    "print(feature_group_name)\n",
    "feature_record = featurestore_runtime_client.get_record(FeatureGroupName=feature_group_name, \n",
    "                                                        RecordIdentifierValueAsString=order_id)\n",
    "print(json.dumps(feature_record, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyathena as pa\n",
    "import pandas as pd\n",
    "\n",
    "# getting the latest fetaure group description\n",
    "feature_group_description = sm_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "# Opening a connection to Athena\n",
    "conn = pa.connect(s3_staging_dir=f's3://{default_bucket}/athena-staging',\n",
    "                    region_name=region)\n",
    "\n",
    "# Getting the table name from the feature group description\n",
    "table_name = feature_group_description['OfflineStoreConfig']['DataCatalogConfig']['TableName']\n",
    "\n",
    "# Querying the table\n",
    "query = f\"\"\"SELECT * FROM \\\"sagemaker_featurestore\\\".\\\"{table_name}\\\" \n",
    "        ORDER BY \"write_time\" DESC\n",
    "        LIMIT 1000;\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c15afef78f50668981ba711ec2661f2a5596a86ec48569938de9a5d0b5f4743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
