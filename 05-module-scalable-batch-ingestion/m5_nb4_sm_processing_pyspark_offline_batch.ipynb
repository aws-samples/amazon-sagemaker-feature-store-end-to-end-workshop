{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Offline Batch ingestion via SageMaker Processing job using Feature Store Spark Connector\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "Please be aware, that you need to run through the notebook [m5_nb0_partition_data](https://github.com/aws-samples/amazon-sagemaker-feature-store-end-to-end-workshop/blob/main/05-module-scalable-batch-ingestion/m5_nb0_partition_data.ipynb) in this section of the workshop, to setup the needed data. \n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark SageMaker Processing script](#Create-PySpark-SageMaker-Processing-script)\n",
    "1. [Run batch ingestion job](#Run-batch-ingestion-job)\n",
    "1. [Verify processing job results](#Verify-processing-job-results)\n",
    "\n",
    "\n",
    "\n",
    "In this example, an alteranative route through the batch ingestion via PySpark Processing containers will be explored to ingest data direclty into the Offline Store. We will use the `.ingest_data()` api instead of the `.put_record()` api. \n",
    "\n",
    "This notebook will display how to use the the [Sagemaker Feature Store Manager](https://pypi.org/project/sagemaker-feature-store-pyspark/). \n",
    "\n",
    "\n",
    "To achieve this, the package [Sagemaker Feature Store Pyspark](https://pypi.org/project/sagemaker-feature-store-pyspark/) is needed. If you want to use other means of Spark, please see the [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html) for further guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from random import randint\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.120.0\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Role = arn:aws:iam::510646607739:role/service-role/AmazonSageMaker-ExecutionRole-20220809T104176\n",
      "Default bucket = sagemaker-us-east-1-510646607739\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = get_execution_role()\n",
    "except:\n",
    "    # for local dev, please set your sagemaker role here\n",
    "    role = 'arn:aws:iam::XXXXXXXX:role/service-role/role-name'\n",
    "logger.info(f'Role = {role}')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "featurestore_runtime_client = sagemaker_session.boto_session.client('sagemaker-featurestore-runtime', region_name=region)\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "logger.info(f'Default bucket = {default_bucket}')\n",
    "prefix = 'sagemaker-feature-store'\n",
    "\n",
    "spark_version = '3.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the packages needed\n",
    "To use the feature store manager directly, we only need two things. \n",
    "- the right `jar` that we can get from [maven](https://mvnrepository.com/artifact/software.amazon.sagemaker.featurestore/sagemaker-feature-store-spark-sdk)\n",
    "- the right python packages that we can get from [PyPi](https://pypi.org/project/sagemaker-feature-store-pyspark/). From PyPi You can download the latest package and copy the `feature-store-manager.py` and `wrapper.py` to your local file system. This has already been done for you in the folder `feature-store-pyspark`. For the packages, please look into the feature_store_pyspark folder. We will need to provide both to the pyspark processor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either get the jars for needed directly from [maven](https://mvnrepository.com/artifact/software.amazon.sagemaker.featurestore/sagemaker-feature-store-spark-sdk), or simply install the feature-store-manager via pip, which will get us the jars without the hassle of copying them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: sagemaker-feature-store-pyspark-3.1 in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: pyathena in /opt/conda/lib/python3.7/site-packages (2.21.0)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /opt/conda/lib/python3.7/site-packages (from pyathena) (1.26.24)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from pyathena) (8.1.0)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /opt/conda/lib/python3.7/site-packages (from pyathena) (1.29.24)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from pyathena) (2022.11.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.26.4->pyathena) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.26.4->pyathena) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore>=1.29.4->pyathena) (1.26.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore>=1.29.4->pyathena) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->pyathena) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker-feature-store-pyspark-{spark_version}\n",
    "%pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p scripts\n",
    "!mkdir -p jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/lib/python3.7/site-packages/feature_store_pyspark/jars/sagemaker-feature-store-spark-sdk.jar'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar_path = !feature-store-pyspark-dependency-jars\n",
    "jar_path = jar_path[0]\n",
    "jar_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the needed packages, the PySpark processing can be created. \n",
    "Not only can we ingest into the feature store direclty, but combine it with data cleaning and feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PySpark Processing script\n",
    "The following script\n",
    "- Creates a data schema for the spark dataframe. \n",
    "- Check the data and the columns for their data type assigments. \n",
    "- Ordinally encodes a column\n",
    "- Scales the data \n",
    "- Transforms the column `purchased_on` to the ML ready feature `n_days_since_last_purchase`.\n",
    "- Repartitions the data \n",
    "- Ingests into the offline feature store\n",
    "\n",
    "If you are only interested in the batch ingestion, please take a look at the `batch_ingest_to_feature_store` method and copare it to the `ingest_to_feature_store` mehtod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/batch_ingest_sm_pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/batch_ingest_sm_pyspark.py\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
    "from feature_store_manager import FeatureStoreManager\n",
    "from pyspark.sql.functions import udf, datediff, to_date, lit, col,isnan, when, count\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from argparse import Namespace, ArgumentParser\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def parse_args() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_processes', type=int, default=1)\n",
    "    parser.add_argument('--num_workers', type=int, default=1)\n",
    "    parser.add_argument('--feature_group_name', type=str)\n",
    "    parser.add_argument('--feature_group_arn', type=str)\n",
    "    parser.add_argument('--target_feature_store_list', type=str)\n",
    "    parser.add_argument('--s3_uri_prefix', type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def transform_row(row) -> list:\n",
    "    columns = list(row.asDict())\n",
    "    record = []\n",
    "    for column in columns:\n",
    "        feature = {'FeatureName': column, 'ValueAsString': str(row[column])}\n",
    "        record.append(feature)\n",
    "    return record\n",
    "\n",
    "def ingest_to_feature_store(args: argparse.Namespace, rows) -> None:\n",
    "    feature_group_name = args.feature_group_name\n",
    "    session = boto3.session.Session()\n",
    "    featurestore_runtime_client = session.client(service_name='sagemaker-featurestore-runtime')\n",
    "    rows = list(rows)\n",
    "    logger.info(f'Ingesting {len(rows)} rows into feature group: {feature_group_name}')\n",
    "    for _, row in enumerate(rows):\n",
    "        record = transform_row(row)\n",
    "        response = featurestore_runtime_client.put_record(FeatureGroupName=feature_group_name, Record=record)\n",
    "        assert response['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "\n",
    "def batch_ingest_to_feature_store(args: argparse.Namespace, df: DataFrame) -> None:\n",
    "    feature_group_name = args.feature_group_name\n",
    "    logger.info(f'Feature Group name supplied is: {feature_group_name}')\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    logger.info(f'Instantiating FeatureStoreManger!')\n",
    "    feature_store_manager=FeatureStoreManager()\n",
    "\n",
    "    logger.info(f'trying to load datatypes directly from Dataframe')\n",
    "\n",
    "    # Load the feature definitions from input schema. The feature definitions can be used to create a feature group\n",
    "    feature_definitions = feature_store_manager.load_feature_definitions_from_schema(df)\n",
    "    logger.info(f'Feature definitions loaded successfully!')\n",
    "    print(feature_definitions)\n",
    "    feature_group_arn = args.feature_group_arn\n",
    "    logger.info(f'Feature Group ARN supplied is: {feature_group_arn}')\n",
    "\n",
    "    # If only OfflineStore is selected, the connector will batch write the data to offline store directly\n",
    "    args.target_feature_store_list = ast.literal_eval(args.target_feature_store_list)\n",
    "    logger.info(f'Ingesting into the following stores: {args.target_feature_store_list}')\n",
    "\n",
    "    feature_store_manager.ingest_data(input_data_frame=df, feature_group_arn=feature_group_arn, target_stores= args.target_feature_store_list) \n",
    "    logger.info(f'Feature Ingestions successful!')\n",
    "\n",
    "\n",
    "def check_data(df: DataFrame) -> None:\n",
    "    # Sanity checking secktion \n",
    "    logger.info(f'First 5 rows of the dataframe for inspection: {df.show(5)}')\n",
    "\n",
    "    df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "    ).show()\n",
    "\n",
    "    # checking for categorical columns\n",
    "    categorical_cols = [field for (field, dataType) in df.dtypes if dataType == 'string']\n",
    "    logger.info(f'Categorical columns: {categorical_cols}')\n",
    "\n",
    "    # checking for numerical columns\n",
    "    numerical_cols = [field for (field, dataType) in df.dtypes if ((dataType == 'double') | (dataType == 'int') | (dataType == 'float'))]\n",
    "    logger.info(f'Numerical columns: {numerical_cols}')\n",
    "\n",
    "    # checking for boolean columns  \n",
    "    boolean_cols = [field for (field, dataType) in df.dtypes if dataType == 'boolean']\n",
    "    logger.info(f'Boolean columns: {boolean_cols}')\n",
    "\n",
    "    # checking for date columns\n",
    "    date_cols = [field for (field, dataType) in df.dtypes if dataType == 'date']\n",
    "    logger.info(f'Date columns: {date_cols}')\n",
    "\n",
    "\n",
    "def scale_col(df: DataFrame, col_name: str) -> DataFrame:\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]), 2), DoubleType())\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=f'{col_name}_vec')\n",
    "    # scale an column col_name with minmax scaler and drop the original column\n",
    "\n",
    "    scaler = MinMaxScaler(inputCol=f'{col_name}_vec', outputCol=f'{col_name}_scaled')\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    df = pipeline.fit(df).transform(df).withColumn(f'{col_name}_scaled', unlist(f'{col_name}_scaled')) \\\n",
    "                                       .drop(f'{col_name}_vec')\n",
    "    df = df.drop(col_name)\n",
    "    df = df.withColumnRenamed(f'{col_name}_scaled', col_name)\n",
    "    return df\n",
    "\n",
    "def ordinal_encode_col(df: DataFrame, col_name: str) -> DataFrame:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f'{col_name}_new')\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    df = df.drop(col_name)\n",
    "    df = df.withColumnRenamed(f'{col_name}_new', col_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_spark_job():\n",
    "\n",
    "    args = parse_args()\n",
    "   \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # set the legacy time parser policy to LEGACY to allow for parsing of dates in the format dd/MM/yyyy HH:mm:ss, which solves backwards compatibility issues to spark 2.4\n",
    "    spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "    logger.info(f'LOGGER: Using Spark-Version:{spark.version}')\n",
    "\n",
    "    # get the total number of cores in the Spark cluster; if developing locally, there might be no executor\n",
    "    try:\n",
    "        spark_context = spark.sparkContext\n",
    "        total_cores = int(spark_context._conf.get('spark.executor.instances')) * int(spark_context._conf.get('spark.executor.cores'))\n",
    "        logger.info(f'LOGGER: Total available cores in the Spark cluster = {total_cores}')\n",
    "    except:\n",
    "        total_cores = 1\n",
    "        logger.info('LOGGER: Could not retrieve number of total cores. Setting total cores to 1')\n",
    "    \n",
    "    logger.info(f'LOGGER: Reading input file from S3. S3 uri is {args.s3_uri_prefix}')\n",
    "\n",
    "    # define the schema of the input data\n",
    "    csvSchema = StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"purchase_amount\", FloatType(), False),\n",
    "        StructField(\"is_reordered\", IntegerType(), False),\n",
    "        StructField(\"purchased_on\", StringType(), False),\n",
    "        StructField(\"event_time\", StringType(), False)])\n",
    "\n",
    "\n",
    "    # read the pyspark dataframe with a schema \n",
    "    df = spark.read.option(\"header\", \"true\").schema(csvSchema).csv(args.s3_uri_prefix)  \n",
    "    \n",
    "    # check the data of the dataframe and write findings to logs for inspection \n",
    "    check_data(df)\n",
    "\n",
    "    # transform 1 - encode boolean to int\n",
    "    df = ordinal_encode_col(df, 'is_reordered')\n",
    "    df = df.withColumn('is_reordered', df['is_reordered'].cast(IntegerType()))\n",
    "\n",
    "    # transform 2 - min max scale `purchase_amount`\n",
    "    df = df.withColumn('purchase_amount', df['purchase_amount'].cast(DoubleType()))\n",
    "    df = scale_col(df, 'purchase_amount')\n",
    "    \n",
    "    # transform 3 - derive `n_days_since_last_purchase` column using the `purchased_on` col\n",
    "    current_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    df = df.withColumn('n_days_since_last_purchase', datediff(to_date(lit(current_date)), to_date('purchased_on', 'yyyy-MM-dd')))\n",
    "    df = df.drop('purchased_on')\n",
    "    df = scale_col(df, 'n_days_since_last_purchase')\n",
    "    \n",
    "    \n",
    "    logger.info(f'LOGGER: Number of partitions = {df.rdd.getNumPartitions()}')\n",
    "    # Rule of thumb heuristic - rely on the product of #executors by #executor.cores, and then multiply that by 3 or 4\n",
    "    df = df.repartition(total_cores * 3)\n",
    "    logger.info(f'Number of partitions after re-partitioning = {df.rdd.getNumPartitions()}')\n",
    "    logger.info(f'Feature Store ingestion start: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n",
    "    batch_ingest_to_feature_store(args, df)\n",
    "    logger.info(f'Feature Store ingestion complete: {datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('BATCH INGESTION - STARTED')\n",
    "    run_spark_job()\n",
    "    logger.info('BATCH INGESTION - COMPLETED')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check that our feature group names, feature definition and our arguments are in line with our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fscw-orders-01-19-17-06'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r orders_feature_group_name\n",
    "s3_uri_prefix = f's3://{default_bucket}/{prefix}/partitions/*'\n",
    "# REUSE orders feature group name from module 1\n",
    "feature_group_name = orders_feature_group_name \n",
    "feature_group_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fscw-orders-01-19-17-06',\n",
       " 'arn:aws:sagemaker:us-east-1:510646607739:feature-group/fscw-orders-01-19-17-06',\n",
       " \"['OfflineStore']\",\n",
       " 's3://sagemaker-us-east-1-510646607739/sagemaker-feature-store/partitions/*')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client=boto3.client('sagemaker')\n",
    "\n",
    "feature_group_description = sm_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "feature_group_arn = feature_group_description['FeatureGroupArn']\n",
    "\n",
    "# please specify what target stores you want to ingest into -> PySpark does not accept list as a parameter\n",
    "target_feature_store_list = \"['OfflineStore']\" # ['OfflineStore', 'OnlineStore'] for both\n",
    "\n",
    "feature_group_name, feature_group_arn, target_feature_store_list, s3_uri_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'FeatureName': 'order_id', 'FeatureType': 'String'},\n",
       " {'FeatureName': 'customer_id', 'FeatureType': 'String'},\n",
       " {'FeatureName': 'product_id', 'FeatureType': 'String'},\n",
       " {'FeatureName': 'purchase_amount', 'FeatureType': 'Fractional'},\n",
       " {'FeatureName': 'is_reordered', 'FeatureType': 'Integral'},\n",
       " {'FeatureName': 'event_time', 'FeatureType': 'String'},\n",
       " {'FeatureName': 'n_days_since_last_purchase', 'FeatureType': 'Fractional'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_description[\"FeatureDefinitions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run batch ingestion job\n",
    "You can check what image URI for the PySpark Processor is the right one for your region via [aws github](https://github.com/aws/sagemaker-spark-container/releases).\n",
    "\n",
    "If so, please specify the image URI via the `image_uri` attribute when instantiating the PySparkProcessor. \n",
    "\n",
    "Alternatively, you can specify the framework version that you would like to use via the `framework_version` attribute. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-preprocessor-2023-02-13-09-56-53-554\n",
      "Inputs:  [{'InputName': 'py-files', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-510646607739/spark-preprocessor-2023-02-13-09-56-53-554/input/py-files', 'LocalPath': '/opt/ml/processing/input/py-files', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'jars', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-510646607739/spark-preprocessor-2023-02-13-09-56-53-554/input/jars', 'LocalPath': '/opt/ml/processing/input/jars', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-510646607739/spark-preprocessor-2023-02-13-09-56-53-554/input/code/batch_ingest_sm_pyspark.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-510646607739/spark-logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      "...........................................................................!"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-preprocessor\",\n",
    "    # image_uri = image_uri,\n",
    "    framework_version=spark_version,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "pyspark_processor.run(submit_app='./scripts/batch_ingest_sm_pyspark.py', \n",
    "                      arguments = ['--feature_group_name', feature_group_name, \n",
    "                                   '--s3_uri_prefix', s3_uri_prefix,\n",
    "                                   '--feature_group_arn', feature_group_arn,\n",
    "                                   '--target_feature_store_list', target_feature_store_list],\n",
    "                      submit_jars=[jar_path],\n",
    "                    submit_py_files=[\n",
    "                            './feature_store_pyspark/feature_store_manager.py',\n",
    "                            './feature_store_pyspark/wrapper.py'\n",
    "                        ],\n",
    "                      spark_event_logs_s3_uri=f's3://{default_bucket}/spark-logs', \n",
    "                      logs=False,\n",
    "                      wait=True)  # set logs=True to disable logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processing job results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "order_id=O89065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscw-orders-01-19-17-06\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"302349b8-fbc4-494d-969c-b29c49dd92b0\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"302349b8-fbc4-494d-969c-b29c49dd92b0\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"15\",\n",
      "      \"date\": \"Mon, 13 Feb 2023 10:03:11 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "order_id =  f'O{randint(1, 100000)}'\n",
    "logger.info(f'order_id={order_id}') \n",
    "print(feature_group_name)\n",
    "feature_record = featurestore_runtime_client.get_record(FeatureGroupName=feature_group_name, \n",
    "                                                        RecordIdentifierValueAsString=order_id)\n",
    "print(json.dumps(feature_record, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>is_reordered</th>\n",
       "      <th>event_time</th>\n",
       "      <th>n_days_since_last_purchase</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O13572</td>\n",
       "      <td>C2674</td>\n",
       "      <td>P2514</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:37.531Z</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O4350</td>\n",
       "      <td>C940</td>\n",
       "      <td>P6345</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:36.888Z</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O44151</td>\n",
       "      <td>C5288</td>\n",
       "      <td>P14795</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:39.759Z</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O92041</td>\n",
       "      <td>C4586</td>\n",
       "      <td>P12821</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:43.175Z</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O52496</td>\n",
       "      <td>C7786</td>\n",
       "      <td>P1289</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:40.345Z</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>O74165</td>\n",
       "      <td>C6823</td>\n",
       "      <td>P16144</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:41.864Z</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>O38351</td>\n",
       "      <td>C5200</td>\n",
       "      <td>P3823</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:39.337Z</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>O18547</td>\n",
       "      <td>C7394</td>\n",
       "      <td>P12260</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:37.876Z</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>O79365</td>\n",
       "      <td>C1507</td>\n",
       "      <td>P4729</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-19T17:05:42.225Z</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>O76400</td>\n",
       "      <td>C3007</td>\n",
       "      <td>P4044</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-19T17:05:42.019Z</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>2023-02-13 10:02:27.432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id customer_id product_id  purchase_amount  is_reordered  \\\n",
       "0     O13572       C2674      P2514             0.59             0   \n",
       "1      O4350        C940      P6345             0.97             0   \n",
       "2     O44151       C5288     P14795             0.89             0   \n",
       "3     O92041       C4586     P12821             0.90             0   \n",
       "4     O52496       C7786      P1289             0.69             0   \n",
       "..       ...         ...        ...              ...           ...   \n",
       "995   O74165       C6823     P16144             0.57             0   \n",
       "996   O38351       C5200      P3823             0.44             0   \n",
       "997   O18547       C7394     P12260             0.53             0   \n",
       "998   O79365       C1507      P4729             0.26             0   \n",
       "999   O76400       C3007      P4044             0.00             1   \n",
       "\n",
       "                   event_time  n_days_since_last_purchase  \\\n",
       "0    2023-01-19T17:05:37.531Z                        0.31   \n",
       "1    2023-01-19T17:05:36.888Z                        0.60   \n",
       "2    2023-01-19T17:05:39.759Z                        0.35   \n",
       "3    2023-01-19T17:05:43.175Z                        0.60   \n",
       "4    2023-01-19T17:05:40.345Z                        0.60   \n",
       "..                        ...                         ...   \n",
       "995  2023-01-19T17:05:41.864Z                        0.43   \n",
       "996  2023-01-19T17:05:39.337Z                        0.62   \n",
       "997  2023-01-19T17:05:37.876Z                        0.45   \n",
       "998  2023-01-19T17:05:42.225Z                        0.32   \n",
       "999  2023-01-19T17:05:42.019Z                        0.33   \n",
       "\n",
       "                 write_time     api_invocation_time  is_deleted  \n",
       "0   2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "1   2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "2   2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "3   2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "4   2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "..                      ...                     ...         ...  \n",
       "995 2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "996 2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "997 2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "998 2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "999 2023-02-13 10:02:27.432 2023-02-13 10:02:27.432       False  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyathena as pa\n",
    "import pandas as pd\n",
    "\n",
    "# getting the latest fetaure group description\n",
    "feature_group_description = sm_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "# Opening a connection to Athena\n",
    "conn = pa.connect(s3_staging_dir=f's3://{default_bucket}/athena-staging',\n",
    "                    region_name=region)\n",
    "\n",
    "# Getting the table name from the feature group description\n",
    "table_name = feature_group_description['OfflineStoreConfig']['DataCatalogConfig']['TableName']\n",
    "\n",
    "# Querying the table\n",
    "query = f\"\"\"SELECT * FROM \\\"sagemaker_featurestore\\\".\\\"{table_name}\\\" \n",
    "        ORDER BY \"write_time\" DESC\n",
    "        LIMIT 1000;\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c15afef78f50668981ba711ec2661f2a5596a86ec48569938de9a5d0b5f4743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
