{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialize Features Offline features to Online Store\n",
    "\n",
    "In this example, we demonstrate how customers can use the [Feature Store Spark Connector](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html) to ingest features directly to the offline store, and incrementally materialize the latest features to the online store.\n",
    "\n",
    "**Note: Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Group\n",
    "\n",
    "First, create a feature group with online and offline stores configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region_name = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "feature_group_name = 'feature-store-offline-to-online-example'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highly recommend storing offline features using the Apache Iceberg table format. If you need to use the Glue table format, please update the variable below to `'Glue'`. \n",
    "\n",
    "For more information on offline store formats, please refer to the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-offline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_format = 'Iceberg' # or 'Glue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.create_feature_group(\n",
    "    FeatureGroupName=feature_group_name,\n",
    "    RecordIdentifierFeatureName='RecordIdentifier',\n",
    "    EventTimeFeatureName='EventTime',\n",
    "    OnlineStoreConfig={\n",
    "        'EnableOnlineStore': True\n",
    "    },\n",
    "    OfflineStoreConfig={\n",
    "        'S3StorageConfig': {\n",
    "            'S3Uri': f's3://{default_bucket}'\n",
    "        },\n",
    "        'TableFormat': table_format\n",
    "    },\n",
    "    FeatureDefinitions=[\n",
    "        {\n",
    "            'FeatureName': 'RecordIdentifier',\n",
    "            'FeatureType': 'Integral'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'Measure',\n",
    "            'FeatureType': 'Fractional'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'EventTime',\n",
    "            'FeatureType': 'String'\n",
    "        }\n",
    "    ],\n",
    "    RoleArn=role\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data to the Offline Store\n",
    "\n",
    "We will create a [SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) which uses the Feature Store Spark Connector to ingests a set of features directly into the offline store.\n",
    "\n",
    "To use the Feature Store Spark Connector in a Processing Job, we recommend extending the prebuilt SageMaker Spark Processing container as shown in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html#:~:text=Installation%20on%20a%20Amazon%20SageMaker%20Processing%20Job\n",
    "). For this example, we will install the Spark Connector to a local directory and submit the required modules and Jar file when we run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_version = '3.1' # MAJOR.MINOR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Spark Connector under `./temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker-feature-store-pyspark-{spark_version} -t ./temp --no-binary :all:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip up the required Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zf = zipfile.ZipFile('feature_store_pyspark.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "\n",
    "for f in os.listdir('./temp/feature_store_pyspark'):\n",
    "    if f.endswith('.py'):\n",
    "        zf.write(os.path.join('./temp/feature_store_pyspark', f), os.path.join('feature_store_pyspark', f))\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `feature_store_pyspark.classpath_jars()` to get the absolute path to the Jar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp import feature_store_pyspark\n",
    "\n",
    "jar_path = feature_store_pyspark.classpath_jars()[0]\n",
    "jar_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a processing job using `scripts/ingest_to_offline_only.py` and include the zipped Python modules and Jar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    max_runtime_in_seconds=1200,\n",
    "    framework_version=spark_version,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app='./scripts/ingest_to_offline_only.py',\n",
    "    arguments=[\n",
    "        '--feature_group_name',\n",
    "        feature_group_name,\n",
    "        '--region_name',\n",
    "        region_name\n",
    "    ],\n",
    "    logs=False,\n",
    "    submit_jars=[jar_path],\n",
    "    submit_py_files=[\n",
    "        './feature_store_pyspark.zip'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialize Latest Features to Online Store\n",
    "\n",
    "Now that our features are ingested to offline store, we can materialize the latest features (for each record identifier) to the online store. To do this, we we will run another Spark Processing Job using `scripts/materialize_to_online.py`. Since the task may need to run on a regular cadence, we can add the processing job to a SageMaker Pipeline. This pipeline can then be scheduled with [Amazon EventBridge](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "pipeline_name = feature_group_name\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    framework_version=spark_version\n",
    ")\n",
    "\n",
    "processor_args = spark_processor.run(\n",
    "    submit_app='./scripts/materialize_to_online.py',\n",
    "    logs=False,\n",
    "    arguments = [\n",
    "        '--table_format',\n",
    "        table_format,\n",
    "        '--feature_group_name',\n",
    "        feature_group_name,\n",
    "        '--region_name',\n",
    "        region_name\n",
    "    ],\n",
    "    submit_jars=[\n",
    "        jar_path\n",
    "    ],\n",
    "    submit_py_files=[\n",
    "        './feature_store_pyspark.zip'\n",
    "    ]\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name='MaterializeToOnlineStore', step_args=processor_args)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[step_process],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "execution.wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the latest features are available in the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_client = boto3.client('sagemaker-featurestore-runtime')\n",
    "fs_client.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName': feature_group_name,\n",
    "            'RecordIdentifiersValueAsString': ['1', '2', '3']\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
