{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Batch Scoring using a trained XGBoost model\n",
    "**This notebook uses the feature set extracted by `module-2` to create a XGBoost based machine learning model for binary classification**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Prepare test data](#Prepare-test-data)\n",
    "1. [Batch Transform](#Batch-Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "After the model is trained, if the goal is to generate predictions on a large dataset where minimizing latency isn't a concern, then SageMaker batch transform is the solution. Functionally, batch transform uses the same mechanics as real-time hosting to generate predictions. It requires a web server that takes in HTTP POST requests a single observation, or mini-batch, at a time. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes.\n",
    "\n",
    "In this example, we will walk through the steps to prepare the batch test dataset from feature store using processing job and perform batch transform with the test data available on Amazon S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    AthenaDatasetDefinition,\n",
    "    DatasetDefinition,\n",
    ")\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utilities import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"__name__\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = get_execution_role()\n",
    "logger.info(f\"Role = {sagemaker_execution_role}\")\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = session.client(service_name=\"sagemaker\")\n",
    "# featurestore_runtime = boto3.client(\"sagemaker-featurestore-runtime\")\n",
    "\n",
    "# feature_store_session = sagemaker.Session(\n",
    "#     boto_session=session,\n",
    "#     sagemaker_client=sagemaker_client,\n",
    "#     sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    "# )\n",
    "\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker-featurestore-workshop\"\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data for batch transform \n",
    "<!-- job using processing job with *AthenaDatasetDefinition* -->\n",
    "We create the test dataset that we use in our batch transform job using [*AthenaDatasetDefinition*](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.dataset_definition.inputs.AthenaDatasetDefinition) API.\n",
    "We follow the steps below to prepare the test dataset for batch transform job:\n",
    "1. firstly generates the list of feature names that we would like to read from the offline feature store by providing the feature group names as a list and a exclude feature list to the *generate_fsets* function. \n",
    "2. Construct an Athena query to read the data from offline feature store and run a SageMaker processing job to transform the data type to 'text/csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate the list of features needed from feature store.\n",
    "\n",
    "We use boto3 sagemaker_client to perform `DescribeFeatureGroup` action to describe a FeatureGroup. The response includes information on the creation time, FeatureGroup name, the unique identifier for each FeatureGroup, and more, for more details of the response syntax, please refer to [document here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeFeatureGroup.html#API_DescribeFeatureGroup_ResponseSyntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "\n",
    "customers_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "products_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=products_feature_group_name\n",
    ")\n",
    "orders_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=orders_feature_group_name\n",
    ")\n",
    "\n",
    "database_name = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Database\"]\n",
    "catalog = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Catalog\"]\n",
    "\n",
    "customers_table = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "products_table = products_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "orders_table = orders_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_fsets = [\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "    \"event_time\",\n",
    "    \"purchase_amount\",\n",
    "    \"n_days_since_last_purchase\",\n",
    "]\n",
    "target_fname = \"is_reordered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(fg_list, exclude_fsets=None, target_fname=None):\n",
    "    _fg_lst = []\n",
    "    for _fg in fg_list:\n",
    "        _fg_tmp = pd.DataFrame(\n",
    "            Utils.describe_feature_group(_fg[\"FeatureGroupName\"])[\"FeatureDefinitions\"]\n",
    "        )\n",
    "        if exclude_fsets:\n",
    "            _fg_tmp = _fg_tmp[~_fg_tmp.FeatureName.isin(exclude_fsets)]\n",
    "\n",
    "        _fg_lst.append(_fg_tmp)\n",
    "    return pd.concat(_fg_lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsets_df = generate_fsets([orders_fg, customers_fg, products_fg], exclude_fsets)\n",
    "features_names = fsets_df.FeatureName.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a SageMaker Processing Job to generate test set for batch job\n",
    "\n",
    "[SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) is the tool to analyze data and evaluate machine learning models. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. In short, SageMaker takes custom script and copies data from Amazon S3 and then pulls a processing container to execute the script which performs all the data processing and other actions as needed.\n",
    "\n",
    "When creating a [Processing job](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateProcessingJob.html), one needs to specify the `ProcessingInputs` parameter which tell the SageMaker service where to get the input data. If the data is already available on S3, we can use the `S3Input` to define the inputs for the processing job. However, in our example, the data is stored in the offline Feature Store, we can use the [DatasetDefinition](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DatasetDefinition.html) which supports the data sources like S3 which can be queried via Athena\n",
    "and Redshift. We use the [AthenaDatasetDefinition](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AthenaDatasetDefinition.html) option, it executes SQL queries and generate datasets to s3 which will be available as the inputs of the processing job.\n",
    "\n",
    "\n",
    "We start by create an Athena query to get the test data from feature store. Note that the first column will be the unique identifier of customer id and the second column is the target value. Note that the query should only take the latest version of any given record that has multiple write times for the same event_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_columns_string = \",\\n    \".join(f'\"{c}\"' for c in features_names)\n",
    "\n",
    "customer_uid = customers_fg[\"RecordIdentifierFeatureName\"]\n",
    "product_uid = products_fg[\"RecordIdentifierFeatureName\"]\n",
    "order_uid = orders_fg[\"RecordIdentifierFeatureName\"]\n",
    "\n",
    "customer_et = customers_fg[\"EventTimeFeatureName\"]\n",
    "product_et = products_fg[\"EventTimeFeatureName\"]\n",
    "order_et = orders_fg[\"EventTimeFeatureName\"]\n",
    "\n",
    "\n",
    "query_string = f\"\"\"WITH customer_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{customer_uid}\"\n",
    "            ORDER BY \"{customer_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{customers_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "product_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{product_uid}\"\n",
    "            ORDER BY \"{product_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{products_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "order_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{order_uid}\"\n",
    "            ORDER BY \"{order_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{orders_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    \"{order_uid}\",\n",
    "    {batch_transform_columns_string}\n",
    "FROM customer_table,\n",
    "    product_table,\n",
    "    order_table\n",
    "WHERE order_table.\"customer_id\" = customer_table.\"customer_id\"\n",
    "    AND order_table.\"product_id\" = product_table.\"product_id\"\n",
    "    AND customer_table.\"rank\" = 1\n",
    "    AND product_table.\"rank\" = 1\n",
    "    AND order_table.\"rank\" = 1\n",
    "\"\"\"\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batchdata_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_execution_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=2,\n",
    "    base_job_name=f\"{prefix}-batch\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_data_path = \"/opt/ml/processing/athena\"\n",
    "athena_output_s3_uri = f\"s3://{default_bucket}/{prefix}/athena/data/\"\n",
    "data_sources = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"athena_dataset\",\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=athena_data_path,\n",
    "            data_distribution_type=\"ShardedByS3Key\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=catalog,\n",
    "                database=database_name,\n",
    "                query_string=query_string,\n",
    "                output_s3_uri=athena_output_s3_uri,\n",
    "                output_format=\"PARQUET\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following processing script reads the Athena query outputs (parquet files) and save as csv files which can be used directly by SageMaker Batch jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_batchdata.py\n",
    "import argparse\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Parse argument variables passed via the CreateDataset processing step\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--athena-data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "dataset_path = Path(\"/opt/ml/processing/output/dataset\")\n",
    "dataset = pd.read_parquet(args.athena_data, engine=\"pyarrow\")\n",
    "\n",
    "# Write train, test splits to output path\n",
    "dataset_output_path = Path(\"/opt/ml/processing/output/dataset\")\n",
    "dataset.to_csv(\n",
    "    dataset_output_path / f\"dataset-{uuid.uuid4()}.csv\", index=False, header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "destination_s3_path = f\"s3://{default_bucket}/{prefix}/{name_from_base('batch')}\"\n",
    "create_batchdata_processor.run(\n",
    "    code=\"create_batchdata.py\",\n",
    "    arguments=[\n",
    "        \"--athena-data\",\n",
    "        athena_data_path,\n",
    "    ],\n",
    "    inputs=data_sources,\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"batch_transform_data\",\n",
    "            source=\"/opt/ml/processing/output/dataset\",\n",
    "            destination=destination_s3_path,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Batch Transform, we introduced 3 new attributes - __input_filter__, __join_source__ and __output_filter__. In the below cell, we use the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to kick-off several Batch Transform jobs using different configurations of these 3 new attributes. Please refer to [this page](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a model based on the pretrained model artifacts on s3\n",
    "Let's first create a model based on the training job from the previous notebook. We can use `describe_training_job` boto3 api call to get the model data url and the container used for the training job. Please note that the prebuilt [XGBoost container](https://github.com/aws/sagemaker-xgboost-container) uses the same container image for training and hosting. However, if you are using other framework containers, such as tensorflow, pytorch and etc., the taining and inference containers are different. For more details about the the available deep learning containers, please refer to the [github page](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_jobName\n",
    "\n",
    "training_job_info = sagemaker_client.describe_training_job(\n",
    "    TrainingJobName=training_jobName\n",
    ")\n",
    "xgb_model_data = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "container_uri = training_job_info['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "xgb_model = Model(\n",
    "    image_uri=container_uri,\n",
    "    model_data=xgb_model_data,\n",
    "    role=sagemaker_execution_role,\n",
    "    name=name_from_base(\"fs-workshop-xgboost-model\"),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Join the input and the prediction results \n",
    "Now, let's associate the prediction results with their corresponding input records. We can also use the __input_filter__ to exclude the order ID column easily and there's no need to have a separate file in S3.\n",
    "\n",
    "* Set __input_filter__ to \"$[2:]\": indicates that we are excluding column 0 (the 'order_id') and column 1 (the target value) before processing the inferences and keeping everything from column 1 to the last column (all the features or predictors)  \n",
    "  \n",
    "  \n",
    "* Set __join_source__ to \"Input\": indicates our desire to join the input data with the inference results  \n",
    "\n",
    "* Leave __output_filter__ to default ('$'), indicating that the joined input and inference results be will saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_transformer = xgb_model.transformer(instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# content_type / accept and split_type / assemble_with are required to use IO joining feature\n",
    "xgb_transformer.assemble_with = \"Line\"\n",
    "xgb_transformer.accept = \"text/csv\"\n",
    "\n",
    "# start a transform job\n",
    "xgb_transformer.transform(\n",
    "    destination_s3_path,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    input_filter=\"$[2:]\",\n",
    "    join_source=\"Input\",\n",
    ")\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def list_s3_files(s3uri):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket = s3.Bucket(parsed_url.netloc)\n",
    "    prefix = parsed_url.path[1:]\n",
    "    return [\n",
    "        dict(bucket_name=k.bucket_name, key=k.key)\n",
    "        for k in bucket.objects.filter(Prefix=prefix)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_list = list_s3_files(xgb_transformer.output_path)\n",
    "output_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    \"s3://{bucket_name}/{key}\".format(**output_file_list[0]),\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
