{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with Amazon SageMaker Offline Feature Store \n",
    "\n",
    "## SageMaker Feature Store Offline SDK enables you to easily build ML-ready datasets from Feature Groups\n",
    "\n",
    "**How to use Amazon SageMaker Feature Store Offline SDK to build ML-ready datasets by loading Pandas dataframes and joining one or more Feature Groups.**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.m5.large`, one of the fast-launch types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Describe Data Sets](#Datasets)\n",
    "1. [Generate Timestamps](#Generate-Timestamps)\n",
    "1. [Enhance Dataframe with Time-Series Data](#Enhance-Dataframe-with-Time-Series-Data)\n",
    "1. [Create Dataset from single Feature Group](#Create-Dataset-from-single-Feature-Group)\n",
    "1. [Demonstrate use of Point-in-Time Join](#Demonstrate-use-of-Point-in-Time-Join)\n",
    "1. [Create Dataset from Joining two Feature Groups](#Create-Dataset-from-Joining-two-Feature-Groups)\n",
    "1. [Create Dataset using Event Time window](#Create-Dataset-using-Event-Time-window)\n",
    "1. [Create Dataset using as-of Timestamp](#Create-Dataset-using-as-of-Timestamp)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "\n",
    "### LICENSE\n",
    "\n",
    "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </br>\n",
    "// SPDX-License-Identifier: MIT-0 </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "SageMaker Feature Store is a purpose-built service to store and retrieve feature data for use by Machine Learning models. The Feature Store provides an Online store capable of low latency high throughput reads and writes, plus an Offline store that provides bulk access to all historical record data, and an automated synchronization of data between the Online and Offline store. \n",
    "\n",
    "The Feature Store Offline SDK provides the ability to quickly and easily build ML-ready datasets for use by ML model training or pre-processing. The SDK makes it easy to build datasets from SQL join, point-in-time accurate join, and event range time frames, all without the need to write any SQL code. This functionality is accessed via the DatasetBuilder class which is the primary entry point for the SDK functionality. This notebook will instruct the user on how to operate the SDK to retrieve feature data in a number of scenarios, including:\n",
    "\n",
    "\n",
    "1. Perform basic feature-level join between one or more Feature Groups and Pandas dataframes. The Offline Store SDK allows you to create datasets using table join logic without the need to write any SQL code. This capability can speed development and time-to-market for your ML use cases.\n",
    "\n",
    "2. Retrieval of point-in-time accurate feature data based on an entity or event-level dataframe. This key capability allows you to retrieve data using “row-level time travel” according to the event times provided to the Dataset Builder (builder).\n",
    "\n",
    "3. Retrieval of feature data with event times within a specified time frame. If you want to retrieve only a subset of your data containing records whose event_time falls within a certain timeframe, you can simply provide the start and end times to the builder.\n",
    "\n",
    "4. Retrieval of feature data with as-of-timestamp (referred to as “time travel”). This capability is useful when you want to access data that represents the state of the datastore at some time in the past. Use cases involve rollback, datastore audits, and avoidance of feature leakage for ML training jobs.\n",
    "\n",
    "\n",
    "In the past, these sorts of data retrieval scenarios required the user to write complex SQL query syntax which would then be submitted to the Amazon Athena service. Athena provides a flexible SQL query engine on top of S3 object storage. While this is still possible, the Offline SDK makes it much easier to build these datasets that incorporate one or more Pandas Dataframes and one or more existing Feature Groups.  For users that prefer to accomplish this type of feature engineering using Apache Spark, SageMaker Feature Store also provides a Spark connector.\n",
    "\n",
    "To demonstrate these capabilities, this notebook sample will create several Feature Groups, and several Pandas Dataframes which are related by a PrimaryKey-to-ForeignKey relationship. We will ingest sample data from a public dataset into the Feature Groups prior to calling methods of the Offline SDK. Then, we will illustrate how to retrieve the ingested features from the multiple Feature Groups and combine them to build feature sets that can be used to train an ML model. \n",
    "\n",
    "This notebook will walk through the following steps:\n",
    "\n",
    "* Create base Dataframe from public Leads dataset using simulated timestamp data, and replicate a handful of records with simulated time-series data (used for point-in-time join)\n",
    "* Create Dataset Builder object with point-in-time join (row-level time travel), and show records that meet that criteria\n",
    "* Compare Athena SQL query results to Offline SDK results, verifying the point-in-time accuracy\n",
    "* Create target Dataframe and ingest public Web Marketing dataset into target Feature Group \n",
    "* Create Dataset Builder object by joining the base and target Feature Groups\n",
    "* Retrieve combined feature data using event time range window, and show records that meet that criteria\n",
    "* Retrieve feature data using as-of timestamp which references the `write_time` datastore attribute\n",
    "* Examine CSV data files, written to S3 by the Offline SDK, which are useful for further processing or feeding directly to training or batch scoring jobs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Leads and Marketing Datasets\n",
    "\n",
    "The use case concerns different marketing activities/metrics captured for the lead by the marketing campaigns. There are two datasets, LeadData and WebMarketingData.\n",
    "\n",
    "LeadData CSV file provides data about the lead and each lead has a unique Lead_ProspectID and Lead_EventTime associated with it. It provides information including job role (JobRole), lead profile (LeadProfile) , whether they used marketing promotion or not (UsedPromo), region (Region), unique Id (Lead_ProspectID) and whether they converted into a sales or not (Converted) etc. This “converted” field is our target feature for model prediction.\n",
    "\n",
    "WebMarketingData CSV file provides data on what all different marketing activities / matrices were performed by the lead under different campaigns run by marketing team. Each activity has a unique Web_ProspectID and Web_EventTime associated with it. It provides data including the last campaign activity performed by the lead (LastCampaignActivity), number of page views per visit (PageViewsPerVisit), total time spend on website (TotalTimeOnWebsite), whether the lead attended the marketing event or not (AttendMarketingEvent), whether the lead viewed the advertisement or not (ViewedAdvertisement) etc.\n",
    "\n",
    "These datasets can also be downloaded from AWS hosted buckets here:\n",
    "\n",
    "Leads Data:\n",
    "https://static.us-east-1.prod.workshops.aws/public/1874ad19-b8dc-4295-923f-a738875ed5c3/static/lab8/LeadData.csv\n",
    "\n",
    "WebMarketing Data: \n",
    "https://static.us-east-1.prod.workshops.aws/public/1874ad19-b8dc-4295-923f-a738875ed5c3/static/lab8/WebMarketingData.csv\n",
    "\n",
    "Note: You do *not* need to download these data files to run this notebook since the CSV files are already stored in the ./data sub-directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Groups using SageMaker SDK\n",
    "\n",
    "Create Feature Groups from Pandas dataframes using SageMaker Feature Store Offline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "\n",
    "import subprocess\n",
    "import importlib\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enforce a minimum required version of SageMaker SDK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_version = sagemaker.__version__\n",
    "\n",
    "major, minor, patch = sm_version.split('.')\n",
    "\n",
    "if int(major) < 2 or int(minor) < 132:\n",
    "    print('Upgrading sagemaker version from: ' + sm_version)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker==2.132.0'])\n",
    "    importlib.reload(sagemaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print('SageMaker version: ' + sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "from datetime import date, datetime, timezone\n",
    "from random import randint\n",
    "\n",
    "from sagemaker.feature_store.feature_store import FeatureStore\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Pandas version {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import local helper code with convenience functions \n",
    "\n",
    "sys.path.append('./code')\n",
    "import sample_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure Session objects \n",
    "\n",
    "region = boto3.Session().region_name\n",
    "print(region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role) \n",
    "\n",
    "# Allocate SageMaker, Feature Store, and S3 clients\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name=\"sagemaker\", region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name=\"sagemaker-featurestore-runtime\",region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create required Feature Store Session object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create FeatureStore session object\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")\n",
    "\n",
    "feature_store = FeatureStore(sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference to default S3 bucket\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "print(f'Using S3 default bucket: {s3_bucket_name}')\n",
    "\n",
    "# Note: Artifacts created by this notebook will be located under this S3 prefix\n",
    "s3_prefix = \"offline-store-sdk-artifacts-\" + \"\".join(random.choice(string.ascii_lowercase) for i in range(10))\n",
    "print(f'S3 prefix for offline store data: {s3_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will generate random timestamps and add them to a Dataframe\n",
    "\n",
    "Feature Groups require a field for EventTime, which represents the actual date/time at which the event occurred. This field must be either type int (unix epoch time) or a string that corresponds to ISO 8601 standard.\n",
    "\n",
    "Note: The timestamps are generated purposely to span within a single calendar year (Jan 2022 ~ Dec 2022).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: initial unix time 1640995200 is 01-01-2022\n",
    "initial_unix_timestamp = 1640995200\n",
    "\n",
    "# Note: delta of 31536000 is one year of unix time\n",
    "one_year_delta = 31536000\n",
    "\n",
    "# Function gen_timestamps generates randomized datetimes using start and delta arguments\n",
    "df_timestamps = sample_helper.gen_timestamps(initial_unix_timestamp, one_year_delta, num=10000)\n",
    "\n",
    "print(df_timestamps.shape)\n",
    "print(df_timestamps.columns)\n",
    "\n",
    "df_timestamps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Lead data from CSV file and load into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create base Dataframe by using Pandas to read CSV file \n",
    "base_data_df = pd.read_csv(\"./data/LeadData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop column to avoid ValueError: Failed to infer Feature type based on dtype bool for column DoNotReachOut.\n",
    "base_data_df = base_data_df.drop('DoNotReachOut', axis=1)  \n",
    "\n",
    "# Set 'Lead_EventTime' column using the generated timestamps\n",
    "base_data_df['Lead_EventTime'] = df_timestamps['Timestamp'].astype('object')\n",
    "\n",
    "# Verify column names for Leads dataset, with Primary Key 'Lead_ProspectID'\n",
    "print(f'Column names: {base_data_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(base_data_df.dtypes)\n",
    "base_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now generate NEWER timestamps for creating time-series data\n",
    "\n",
    "Note: Point-in-Time-Accurate-Join method is useful to operate over time-series data. Therefore, we clone an existing record from the dataset, create new (fake) values for `LeadSource` to easily identify these new records, and then append these additional records to the original dataframe. We also generate newer timestamps (from Jan 2023) for the new sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: unix time 1672531200 is 01-01-2023\n",
    "jan2023_unix_timestamp = 1672531200\n",
    "\n",
    "# Note: delta of 2592000 is one MONTH of unix time\n",
    "one_month_delta = 2592000\n",
    "\n",
    "new_timestamps_df = sample_helper.gen_timestamps(jan2023_unix_timestamp, one_month_delta, num=100)\n",
    "\n",
    "new_timestamps_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enhance Dataframe with Time-Series Data\n",
    "\n",
    "We select existing records and create copies with different timestamps for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Locate existing records to replicate \n",
    "\n",
    "sample_df1 = base_data_df.iloc[[0]]\n",
    "sample_rec1 = sample_df1.iloc[0]\n",
    "record_id1 = sample_rec1['Lead_ProspectID']\n",
    "print(f'Sample record ID[0]: {record_id1}')\n",
    "\n",
    "sample_df2 = base_data_df.iloc[[1]]\n",
    "sample_rec2 = sample_df2.iloc[0]\n",
    "record_id2 = sample_rec2['Lead_ProspectID']\n",
    "print(f'Sample record ID[1]: {record_id2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new dataframes for time-series data using the new timestamps\n",
    "\n",
    "Note: The code below calls methods in supplemental Python file under the ./code directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate new records for time-series data (sample #1)\n",
    "\n",
    "new_records_df1 = sample_helper.gen_new_records(sample_rec1, new_timestamps_df, num=10)\n",
    "print(f'NewRecTypes DF1: {new_records_df1.shape} types: {new_records_df1.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate new records for time-series data (sample #2)\n",
    "\n",
    "new_records_df2 = sample_helper.gen_new_records(sample_rec2, new_timestamps_df, num=10)\n",
    "print(f'NewRecTypes DF2: {new_records_df2.shape} types: {new_records_df2.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View new records for time-series #1\n",
    "print(new_records_df1.shape)\n",
    "new_records_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View new records for time-series #2\n",
    "print(new_records_df2.shape)\n",
    "new_records_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate new records to copy of base Dataframe\n",
    "\n",
    "updated_base_df = pd.concat([base_data_df, new_records_df1, new_records_df2], ignore_index=True)\n",
    "\n",
    "print(base_data_df.shape)\n",
    "print(updated_base_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FeatureGroup for base dataframe\n",
    "\n",
    "We create the initial (base) Feature Group by choosing a name, loading feature definitions from the Pandas dataframe, and then calling `FeatureGroup.create`, providing the names for the two required attributes (`record_identifier_name` and `event_time_feature_name`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Objects created under bucket: ' + s3_bucket_name)\n",
    "\n",
    "# Create Feature Group:\n",
    "# Note: the base_name becomes the name of the FeatureGroup, and can be changed by user\n",
    "base_name = \"off-sdk-fg-lead\"   # No underscores\n",
    "\n",
    "base_fg = FeatureGroup(name=base_name, sagemaker_session=feature_store_session)\n",
    "\n",
    "base_fg.load_feature_definitions(data_frame=updated_base_df) \n",
    "\n",
    "base_fg.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{s3_prefix}\",\n",
    "    record_identifier_name=\"Lead_ProspectID\",\n",
    "    event_time_feature_name=\"Lead_EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,  # False to disable Online store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_helper.wait_for_feature_group_creation_complete(feature_group=base_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Offline Store S3 Uri\n",
    "\n",
    "We call `FeatureGroup.describe` method to lookup config items for the Feature Group. In this case, we lookup the full S3 Uri prefix where Offline Store files are written, and remove the bucket name to be used as S3 prefix further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use describe to lookup Offline Store S3 URI\n",
    "\n",
    "base_fg_resolved_output_s3_uri = base_fg.describe()[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"ResolvedOutputS3Uri\"]\n",
    "\n",
    "base_fg_s3_prefix = base_fg_resolved_output_s3_uri.replace(f\"s3://{s3_bucket_name}/\", \"\")\n",
    "print(base_fg_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data from Dataframe into Feature Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for data ingested into Feature Group to replicate to Offline Store\n",
    "\n",
    "When data is written to a Feature Group (e.g. using `put_record` or `ingest`), it lands in the Online Store initially. Next, this data is replicated to the Offline Store hosted on S3, where it can be queried and loaded for pre-processing or model training. This replication can take anywhere from 5 minutes up to 15 minutes. The wait function below will test and wait for data to appear in the S3 Offline Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_fg.ingest(data_frame=updated_base_df, max_workers=3, wait=True)\n",
    "\n",
    "sample_helper.wait_for_feature_group_data_ingest(s3_bucket_name, base_fg_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset from single Feature Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a Feature Store Dataset from one or multiple existing Feature Groups. Here we call the first new SDK method `FeatureStore.create_dataset` which wraps an underlying call to the [DatasetBuilder](https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#dataset-builder) class to generate our first dataset. \n",
    "\n",
    "Note: The `to_csv_file` method returns a tuple, consisting of the S3 location of the CSV file, and the underlying query executed to produce the datast. The execution query can be inspected for debugging purposes, or can be provided as an Athena query if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset from single (base) FeatureGroup \n",
    "\n",
    "ds1_builder = feature_store.create_dataset(\n",
    "\tbase=base_fg,\n",
    "\toutput_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ")\n",
    "\n",
    "# Returns: tuple (CSV file, SQL query)\n",
    "csv_file, query = ds1_builder.to_csv_file()\n",
    "\n",
    "# Show S3 location of CSV file\n",
    "print(f'CSV file: {csv_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset from Feature Group and Pandas Dataframe\n",
    "\n",
    "Another way to create a Feature Store Dataset is to reference a Pandas dataframe and an existing Feature Group. In this case, a SQL Join operation is performed between the existing feature group and the dataframe, using the `record_identifier_feature_name` attribute as join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset from Pandas dataframe\n",
    "\n",
    "ds2_builder = feature_store.create_dataset(\n",
    "    base=new_records_df2,  # Pandas dataframe\n",
    "    event_time_identifier_feature_name=\"Lead_EventTime\", \n",
    "    record_identifier_feature_name=\"Lead_ProspectID\",\n",
    "    output_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ").with_feature_group(base_fg, \"Lead_ProspectID\", [\"LeadSource\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dataframe from dataset\n",
    "ds2_df, ds2_query = ds2_builder.to_dataframe()\n",
    "\n",
    "print(ds2_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit Pandas query for matching record ID\n",
    "query = f\"LeadSource == 'NewLeadSource_0'\"\n",
    "\n",
    "leadsource_match = ds2_df.query(query, inplace=False)\n",
    "print(leadsource_match.shape)\n",
    "\n",
    "leadsource_match.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate use of Point-in-Time Join\n",
    "\n",
    "We reference the two record_id's from the extra sample records generated above to perform the point-in-time join. These new sample records create a mini-set of time-series data where the `Lead_EventTime` spans a single month (Jan 2023). Below, we create an entity dataframe with two event times that indicate the \"cut-off\" time. When we enable the `point_in_time_accurate_join` in the `create_dataset` call below, the internal query will exclude all records with timestamps LATER then the cut-off times supplied. The entity dataframe with the cut-off event times is submitted as the `base` dataframe in the DatasetBuilder constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Events (entity table) dataframe to pass Timestamp for Point-in-Time Join\n",
    "\n",
    "events = [['2023-01-20T00:00:00Z', record_id1],\n",
    "          ['2023-01-15T00:00:00Z', record_id2]]\n",
    "\n",
    "print(events)\n",
    "\n",
    "df_events = pd.DataFrame(events, columns=['Event_Time', 'Lead_ProspectID']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dataset Builder using point-in-time-accurate-join function\n",
    "\n",
    "pit_builder = feature_store.create_dataset(\n",
    "    base=df_events, \n",
    "    event_time_identifier_feature_name='Event_Time', \n",
    "    record_identifier_feature_name='Lead_ProspectID',\n",
    "    output_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ").with_feature_group(base_fg, \"Lead_ProspectID\"\n",
    ").point_in_time_accurate_join(\n",
    ").with_number_of_recent_records_by_record_identifier(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the results returned by the point-in-time join\n",
    "\n",
    "Notice that there are only two records in the dataframe returned by the point-in-time join. This is because we only submitted two records in the entity dataframe, one for each record_id we want to retrieve. The entity dataframe includes the record_id (primary key) to match and the event time for each key, which is used to exclude newer records. This means that a record's event time (stored in `Lead_Eventtime` field) must contain a value that is less-than the cut-off time. \n",
    "\n",
    "Additionally, we only retrieve the latest record that meets this criteria since we have applied the `with_number_of_recent_records_by_record_identifier` method. When used in conjunction with `point_in_time_accurate_join` method, this allows the caller to specify how many records to return from those that meet the point-in-time join criteria. \n",
    "\n",
    "Notice also that each of the records returned from the point-in-time join contain the new (fake) LeadSource values, which proves they were part of the mini-set of time-series data added to the original dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export result set to Dataframe and Query string\n",
    "pit_df, pit_query = pit_builder.to_dataframe()\n",
    "\n",
    "print(pit_df.shape)\n",
    "pit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can retrieve CSV file from S3 to validate record data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write point-in-time-accurate-join result set to S3 file\n",
    "\n",
    "# Returns: tuple (CSV file, SQL query)\n",
    "pit_csv, pit_query = pit_builder.to_csv_file()\n",
    "pit_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now let's build an Athena Query to compare the results of the Offline SDK\n",
    "\n",
    "First, we build a standard Athena query using a SELECT statement that returns ALL records for the given record_id, since we have not provided a WHERE clause to restrict the results set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Athena Query and retrieve table name\n",
    "lead_query = base_fg.athena_query()\n",
    "print(lead_query)\n",
    "\n",
    "lead_table = lead_query.table_name\n",
    "print(lead_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Athena query string and output location\n",
    "lead_query_recid2 = f'SELECT * FROM \"sagemaker_featurestore\".\"{lead_table}\" ' \\\n",
    "               f'WHERE \"{lead_table}\".\"Lead_ProspectID\" = \\'{record_id2}\\' '\n",
    "\n",
    "print(lead_query_recid2)\n",
    "\n",
    "output_location = f's3://{s3_bucket_name}/{s3_prefix}/athena_query_results/'\n",
    "print(output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lead_query.run(query_string=lead_query_recid2, output_location=output_location)\n",
    "lead_query.wait()\n",
    "\n",
    "athena_results_df = lead_query.as_dataframe()\n",
    "print(athena_results_df.shape)\n",
    "athena_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Athena Query Results\n",
    "\n",
    "Notice that the Athena query results also contain fields attached by the Feature Store, such as `write_time` and `is_deleted`. These fields are added to the record as it is written to the datastore, and provide additional ways to query or sort the record data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare records from Athena Query to Point-in-Time results above\n",
    "\n",
    "Notice that the Athena query from the SELECT statement above does not contain any point-in-time join semantics, so it returns all records that match the specified record_id (`Lead_ProspectID`). Next, we use Pandas to sort the Athena results by event times for easy comparison. Notice that the records with timestamps LATER than the events specified in the entity dataframe (e.g. '2023-01-15T00:00:00Z') submitted to the `point_in_time_accurate_join` do NOT show up in the point-in-time results. Since we additionally specified that we only want a single record from the `create_dataset` code above, we only get the latest record prior to the \"cut off\" time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort records from Athena Query by lead_eventtime column\n",
    "sort_athena_df = athena_results_df.sort_values(by='lead_eventtime', axis=0, ascending=True, inplace=False)\n",
    "\n",
    "for idx, row in sort_athena_df.iterrows():\n",
    "    lead_rec_id = row['lead_prospectid']\n",
    "    lead_source = row['leadsource']\n",
    "    datetime_str = row['lead_eventtime']\n",
    "    print(f'Lead_ProspectID: {lead_rec_id} Lead_Source: {lead_source} DateTime: {datetime_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Target Feature Group from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Feature Group:\n",
    "# Note: the target_name becomes the name of the FeatureGroup, and can be changed by user\n",
    "\n",
    "target_name = \"off-sdk-fg-webmarketing\"\n",
    "\n",
    "target_fg = FeatureGroup(name=target_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create target Dataframe by using Pandas to read CSV file \n",
    "target_data_df = pd.read_csv(\"./data/WebMarketingData.csv\")\n",
    "\n",
    "# Event time type should be either Fractional(Unix timestamp in seconds) or String (ISO-8601 format) type\n",
    "target_data_df['Web_EventTime'] = df_timestamps['Timestamp']\n",
    "\n",
    "# Verify column names for WebMarketing dataset, with Primary Key 'Web_ProspectID'\n",
    "print(f'Column names: {target_data_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_fg.load_feature_definitions(data_frame=target_data_df)\n",
    "\n",
    "target_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: we disable the Online Store below, this notebook will focus only on Offline Store\n",
    "\n",
    "target_fg.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{s3_prefix}\",\n",
    "    record_identifier_name=\"Web_ProspectID\",\n",
    "    event_time_feature_name=\"Web_EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,  # Enable Online store for get_record and put_record\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest data into target Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_helper.wait_for_feature_group_creation_complete(feature_group=target_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_fg_resolved_output_s3_uri = target_fg.describe()[\"OfflineStoreConfig\"][\"S3StorageConfig\"][\"ResolvedOutputS3Uri\"]\n",
    "\n",
    "target_fg_s3_prefix = target_fg_resolved_output_s3_uri.replace(f\"s3://{s3_bucket_name}/\", \"\")\n",
    "print(target_fg_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for data ingested into Feature Group to replicate to Offline Store\n",
    "\n",
    "When data is written to a Feature Group (e.g. using `put_record` or `ingest`), it lands in the Online Store initially. Next, this data is replicated to the Offline Store hosted on S3, where it can be queried and loaded for pre-processing or model training. This replication can take anywhere from 5 minutes up to 15 minutes. The wait function below will test and wait for data to appear in the S3 Offline Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_fg.ingest(data_frame=target_data_df, max_workers=3, wait=True)\n",
    "\n",
    "sample_helper.wait_for_feature_group_data_ingest(s3_bucket_name, target_fg_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We read a sample record (index 9999) and write a new version\n",
    "\n",
    "This updated record will have a later `write_time` than the original record from the table. In the create_dataset `as_of` section below, we will use a carefully constructed datetime (cut-off time) to exclude this later version of the record from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_rec9999 = base_data_df.iloc[9999]\n",
    "\n",
    "record_id9999 = sample_rec9999['Lead_ProspectID']\n",
    "print(f'Sample record ID[9999]: {record_id9999}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `get_record` to retrieve record and `put_record` to write new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Using record ID: {record_id9999}')\n",
    "colnames = list(target_data_df.columns.values)\n",
    "\n",
    "rec = target_fg.get_record(record_id9999, colnames)\n",
    "print(rec)\n",
    "\n",
    "now_ts = int(round(time.time()))\n",
    "now_iso8601 = sample_helper.convert_timestamp_to_iso8601(now_ts)\n",
    "print(now_iso8601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we also update the `TotalWebVisits` field to `9999` to make it easy to identify this additional record in the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_rec = []\n",
    "for kv_field in rec:\n",
    "    if kv_field['FeatureName'] == 'Web_EventTime':\n",
    "        kv_field['ValueAsString'] = now_iso8601\n",
    "    if kv_field['FeatureName'] == 'TotalWebVisits':\n",
    "        kv_field['ValueAsString'] = '9999'\n",
    "    updated_rec.append(kv_field)\n",
    "\n",
    "print(updated_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write updated record\n",
    "featurestore_runtime.put_record(FeatureGroupName=target_fg.name, Record=updated_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset from Joining two Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use the `FeatureStore.create_dataset` and `with_feature_group` methods to build a dataset by joining two existing feature groups. To create the dataset, we provide the feature group references, the join key (referred to in the `target_feature_name_in_base` argument), and the set of features to include in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset by joining two Feature Groups \n",
    "\n",
    "join_builder = feature_store.create_dataset(\n",
    "\tbase=base_fg,\n",
    "\toutput_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ").with_feature_group(\n",
    "        feature_group=target_fg, \n",
    "        target_feature_name_in_base=\"Lead_ProspectID\", \n",
    "        included_feature_names=[\"Web_ProspectID\", 'LastCampaignActivity', 'PageViewsPerVisit',\n",
    "       'TotalTimeOnWebsite', 'TotalWebVisits', 'AttendedMarketingEvent',\n",
    "       'OrganicSearch', 'ViewedAdvertisement']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the builder to generate a Pandas Dataframe\n",
    "join_df, join_query_str = join_builder.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the joined result set includes columns from both the Leads and WebMarketing datasets\n",
    "\n",
    "The DatasetBuilder functions will modify feature names as needed to avoid name collisions. For example, notice that features from the WebMarketing feature group below are appended, such as `Web_ProspectID.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confirm list of columns and dimensions in the resultset\n",
    "print(join_df.columns)\n",
    "print(join_df.shape)\n",
    "\n",
    "# Display few records from join dataframe\n",
    "join_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results to S3 as CSV file\n",
    "\n",
    "To validate the join operation from the `with_feature_group` call above, we can use the builder object method `to_csv_file` to write the results to S3. We can download this CSV file to verify that the join was performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns: tuple (CSV file, SQL query)\n",
    "s3_file, s3_query_str = join_builder.to_csv_file()\n",
    "print(s3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset using Event Time window\n",
    "\n",
    "Setup the Event Time range window by converting Unix epoch times to Python datetimes. The `with_event_time_range` method allows the user to specify a time range (start and end) to apply to the WHERE clause of the query, thus providing a time constraint to records returned in the result set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Event Time window: seconds of unix epoch time\n",
    "\n",
    "# Start at 07/01/2022 and set time window to one day\n",
    "start_ts = 1656633600\n",
    "time_window = 86400\n",
    "\n",
    "# Using hard-coded timestamps from dataset, then adding time window\n",
    "datetime_start = datetime.fromtimestamp(start_ts)\n",
    "datetime_end = datetime.fromtimestamp(start_ts+time_window)\n",
    "print(f'Setting retrieval time window: {datetime_start} until {datetime_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset with specified event_time window\n",
    "\n",
    "time_window_builder = feature_store.create_dataset(\n",
    "\tbase=base_fg, \n",
    "\toutput_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ").with_feature_group(\n",
    "        feature_group=target_fg, \n",
    "        target_feature_name_in_base=\"Lead_ProspectID\", \n",
    "        included_feature_names=[\"Web_ProspectID\", 'LastCampaignActivity', 'PageViewsPerVisit',\n",
    "       'TotalTimeOnWebsite', 'TotalWebVisits', 'AttendedMarketingEvent',\n",
    "       'OrganicSearch', 'ViewedAdvertisement']\n",
    ").with_event_time_range(\n",
    "        starting_timestamp=datetime_start, \n",
    "        ending_timestamp=datetime_end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export to dataframe\n",
    "time_window_df, time_window_query_str = time_window_builder.to_dataframe()\n",
    "\n",
    "# Confirm list of columns in the result-set\n",
    "print(time_window_df.columns)\n",
    "\n",
    "# Confirm that only subset of records in result-set due to event_time window constraint\n",
    "print(time_window_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dump resulting dataframe\n",
    "time_window_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can directly view the SQL code produced by the Offline Store SDK\n",
    "\n",
    "Whenever the `to_csv_file` or `to_dataframe` methods are called, a tuple is returned where the second member is the actual SQL query produced by the Offline SDK engine. This SQL code can be reviewed to better understand how the various methods impact the executed query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show SQL used to create dataset, using the event_time window constraint \n",
    "print(time_window_query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset using as-of Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we demonstrate how to build a dataset and perform feature retrieval with the `as_of` method, which takes a single timestamp argument. \n",
    "\n",
    "Please Note: The `as_of` method applies the time constraint to the `write_time` internal field which is automatically generated by the Feature Store service. The `write_time` field represents the actual time the record is written to the datastore (as opposed to the client-provided `event_time`). The `write_time` field, along with other datastore-driven fields, is only retrievable using Athena, so we will construct Athena queries below to validate the results returned by the `as-of` method of the DatasetBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Athena Query and retrieve table name\n",
    "webmark_query = target_fg.athena_query()\n",
    "print(webmark_query)\n",
    "\n",
    "webmark_table = webmark_query.table_name\n",
    "print(webmark_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose a cut-off time from one of our two sample records for index 9999\n",
    "\n",
    "Using an Athena query, we pull all records from the WebMarketing table with a matching record ID (using index 9999). We then sort them according to `write_time` and choose the latest value as the cut-off time for the `as_of` dataset. The latest value will correspond with the extra record written with `put_record` method above. We purposely wrote this record later so its `write_time` value would be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Athena query string and output location\n",
    "webmark_query_recids = f'SELECT * FROM \"sagemaker_featurestore\".\"{webmark_table}\" ' \\\n",
    "               f'WHERE \"{webmark_table}\".\"Web_ProspectID\" = \\'{record_id9999}\\' ' \n",
    "print(webmark_query_recids)\n",
    "\n",
    "output_location = f's3://{s3_bucket_name}/{s3_prefix}/athena_query_results/'\n",
    "print(output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to wait for last record to replicate to Offline Store\n",
    "\n",
    "Whenever a record is written to the Online Store (using df.ingest or put_record), it is replicated after a short delay to the Offline storage on S3. Below, we run an Athena query in a loop that waits for the most recent record to arrive in the Offline Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# While loop to run Athena query and count number of rows returned\n",
    "\n",
    "num_waits = 0\n",
    "while num_waits < 10:\n",
    "    webmark_query.run(query_string=webmark_query_recids, output_location=output_location)\n",
    "    webmark_query.wait()\n",
    "    athena_web_results_df = webmark_query.as_dataframe()\n",
    "    num_rows, num_cols = athena_web_results_df.shape\n",
    "    print(f'num_rows found: {num_rows}')\n",
    "    if (num_rows > 1):\n",
    "        break   # break out of while loop\n",
    "    print(\"Waiting for latest record to arrive in offline store...\")\n",
    "    time.sleep(60)\n",
    "    num_waits += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort records from Athena query according to `write_time` field (descending)\n",
    "\n",
    "sort_athena_df2 = athena_web_results_df.sort_values(by='write_time', axis=0, ascending=False, inplace=False)\n",
    "sort_athena_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use first record (with latest datetime) to create the cut-off value \n",
    "webmark_rec = sort_athena_df2.iloc[0]\n",
    "\n",
    "webmark_write_time = webmark_rec['write_time']\n",
    "print(f'write_time: {webmark_write_time}')\n",
    "\n",
    "webmark_write_time_truncated = webmark_write_time[:-4]\n",
    "print(f'write_time truncated: {webmark_write_time_truncated}')\n",
    "\n",
    "datetime_format = \"%Y-%m-%d\" + \" \" + \"%H:%M:%S\" \n",
    "\n",
    "asof_cutoff_datetime = datetime.strptime(webmark_write_time_truncated, datetime_format)\n",
    "print(f'as-of cut-off datetime: {asof_cutoff_datetime}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset using as-of timestamp\n",
    "print(f'using cut-off time: {asof_cutoff_datetime}')\n",
    "\n",
    "as_of_builder = feature_store.create_dataset(\n",
    "\tbase=base_fg, \n",
    "\toutput_path=f\"s3://{s3_bucket_name}/{s3_prefix}/dataset_query_results\"\n",
    ").with_feature_group(\n",
    "        feature_group=target_fg, \n",
    "        target_feature_name_in_base='Lead_ProspectID', \n",
    "        included_feature_names=['Web_ProspectID', 'Web_EventTime', 'TotalWebVisits']\n",
    ").as_of(asof_cutoff_datetime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval using the `as_of` method excludes records later than the supplied cut-off time from the result set\n",
    "\n",
    "Our JOIN query for the entire dataset, prior to adding the new record above, resulted in 10,020 records (the intial 10,000 plus our time-series records). We then called `get_record` followed by `put_record` to create an additional modified version of one record (using record index 9999). Applying the `as_of` criteria with a specific cut-off time eliminated the most recent record, the one written with `put_record` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "as_of_df, as_of_query = as_of_builder.to_dataframe()\n",
    "\n",
    "print(as_of_df.shape)\n",
    "print(as_of_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pandas query, we show that the later record is not part of the `as_of` dataframe\n",
    "\n",
    "Here we run a Pandas query on the `as_of_df` dataframe with a query condition that matches the given record ID. The Pandas query only returns one record, the one corresponding to the earlier timestamp (the record with later `write_time` was excluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query condition using record index 9999\n",
    "\n",
    "condition_id = f\"`Web_ProspectID.1` == '{record_id9999}'\"\n",
    "print(condition_id)\n",
    "\n",
    "subset_asof_df = as_of_df.query(condition_id, inplace=False)\n",
    "print(subset_asof_df.shape)\n",
    "\n",
    "subset_asof_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "The cell below will cleanup resources created by this notebook, including the Feature Groups and (optionally) the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete feature groups created by this notebook\n",
    "\n",
    "# Uncomment and run these lines of code to delete the feature groups\n",
    "#base_fg.delete()\n",
    "#target_fg.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
