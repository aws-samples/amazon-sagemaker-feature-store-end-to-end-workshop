{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Working with Offline Feature Store \n",
    "## Features Store Dataset Extraction\n",
    "**How to use Amazon SageMaker Feature Store to retrieve and share machine learning (ML) features in order to build feature sets that can be used for training a ML model.**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.m5.4xlarge`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Data Query](#Data-Query)\n",
    "    1. [Look in S3 console](#Look-in-S3-console)\n",
    "    1. [Query in Athena console](#Query-in-Athena-console)\n",
    "    1. [Query using SageMaker SDK](#Query-using-SageMaker-SDK)\n",
    "1. [Extract a feature set](#Extract-a-feature-set)    \n",
    "1. [Optional queries and validations](#Optional-queries-and-validations)\n",
    "    1. [Browse the set of offline store files in the S3 console](#Browse-the-set-of-offline-store-files-in-the-S3-console.)\n",
    "    1. [See the Glue tables that are used for Athena queries](#See-the-Glue-tables-that-are-used-for-Athena-queries.)\n",
    "    1. [Examine contents of a sample offline store Parquet file](#Examine-contents-of-a-sample-offline-store-Parquet-file.)\n",
    "    1. [Count the rows in an offline store](#Count-the-rows-in-an-offline-store.)\n",
    "    1. [Get a random sample of offline store rows](#Get-a-random-sample-of-offline-store-rows.)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In previous module (`Module-1 Introduction to SageMaker Feature Store`), we demonstrated how to create multiple features groups inside a Amazon SageMaker Feature Store and ingest data into it.\n",
    "\n",
    "In this notebook, we will illustrate how to retrieve the ingested features from the multiple feature groups and combine them to build feature sets that can be used to train a ML model. We will cover the following aspects:\n",
    "\n",
    "* Look at data via S3 console (Offline feature store)\n",
    "* Athena query for dataset extraction - shown via Athena console\n",
    "* Athena query for dataset extraction (programmatically using SageMaker SDK)\n",
    "* Extract training dataset and persist to S3\n",
    "* Some additional queries to visualize a Parquet file, count the rows and get a random sample of offline store\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Query\n",
    "\n",
    "Before starting, in order to get into Feature Store in SageMaker Studio in the left side menu you need to select the SageMaker Components and registries, then select Feature Store from the list of components and click on the Feature store button.\n",
    "\n",
    "![Feature Store](../images/FS_0.png \"Feature Store\")\n",
    "\n",
    "First, let's start by looking at the feature store data. \n",
    "\n",
    "Once the features groups are created (as demonstrated in `Module-1`), we should be able to see the three features groups - `orders`, `products` and `customers` from the SageMaker Studio UI.\n",
    "\n",
    "![Features Groups](../images/FS1.png \"Features Groups\")\n",
    "\n",
    "Double click on one feature group to open a new correspondent tab in Studio. Inside one particular feature group, we can check the location of the offline data by looking for s3 on the bottom right like in the following image. \n",
    "\n",
    "![Orders FG](../images/FS2.png \"Orders FG\")\n",
    "\n",
    "### Look in S3 console\n",
    "\n",
    "In a different tab please open S3 console in your AWS account. The location of the offline feature store data is in S3 inside the default S3 bucket and prefix will respect the following format:\n",
    "\n",
    "s3://DEFAULT_BUCKET/sagemaker-feature-store/ACCOUNT_ID/sagemaker/REGION/offline-store/\n",
    "\n",
    "![S3 Location](../images/S3_1.png \"S3 Location\")\n",
    "\n",
    "We notice that in S3 we have a timestamp suffix added for each feature group name and this corresponds to what is displayed in Athena console as tables.\n",
    "\n",
    "You can get the S3 URL from your environment after launching the cell referenced by: [Browse the set of offline store files in the S3 console.](#Browse-the-set-of-offline-store-files-in-the-S3-console.)\n",
    "\n",
    "Inside each feature group, we have a `data` subdirectory followed by directories partitioned by year/month/day/hour of Parquet files.\n",
    "\n",
    "![S3 Files](../images/S3_2.png \"S3 Files\")\n",
    "\n",
    "You will can get a look into a sample Parquet file after launching the cell referenced by: [Examine contents of a sample offline store Parquet file](#Examine-contents-of-a-sample-offline-store-Parquet-file.)\n",
    "\n",
    "---\n",
    "### Query in Athena console\n",
    "\n",
    "If it is for the first time we are launching Athena in AWS console we need to click on `Get Started` button and then before we run the first query we need to set up a query results location in Amazon S3. For simplicity, we can choose the same default SageMaker bucket that is used by Feature Store.\n",
    "\n",
    "![Athena results location](../images/AthenaSetupMessage.png \"Athena results location\")\n",
    "\n",
    "After setting the query results location, on the left panel we need to select the `AwsDataCatalog` as Data source and the `sagemaker_featurestore` as Database.\n",
    "\n",
    "We can run now run a query for the offline feature store data in Athena. To select the entries from the orders feature group we use the following SQL query. You will need to replace the orders table name with the corresponded value from your environment.\n",
    "\n",
    "```sql\n",
    "select * from \"<orders-feature-group-table-name>\"\n",
    "limit 20\n",
    "```\n",
    "\n",
    "![Athena Orders](../images/AthenaOrders.png \"Athena Orders\")\n",
    "\n",
    "You can notice that Feature Store adds the `write_time`, `api_invocation_time` and `is_deleted` fields to the offline store.\n",
    "\n",
    "Similarly, we can query the products and the customers feature group data by replacing the table name with the correspondent value. \n",
    "\n",
    "```sql\n",
    "select * from \"<products-feature-group-table-name>\"\n",
    "limit 20\n",
    "select * from \"<customers-feature-group-table-name>\"\n",
    "limit 20\n",
    "```\n",
    "\n",
    "Since our three features groups are related, we can do a join query by grouping all the information by product ID and by customer ID like here below. Please make sure you are replacing the table names with the corresponding values from your environment.\n",
    "\n",
    "```sql\n",
    "select *\n",
    "FROM\n",
    "  \"<orders-feature-group-table-name>\"\n",
    ", \"<products-feature-group-table-name>\"  \n",
    ", \"<customers-feature-group-table-name>\"\n",
    "WHERE (\"<orders-feature-group-table-name>\".\"customer_id\" = \"<customers-feature-group-table-name>\".\"customer_id\")\n",
    "AND (\"<orders-feature-group-table-name>\".\"product_id\" = \"<products-feature-group-table-name>\".\"product_id\")\n",
    "limit 20\n",
    "```\n",
    "\n",
    "![Athena Join](../images/AthenaJoin.png \"Athena Join\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query using SageMaker SDK\n",
    "\n",
    "Extract the data from Feature Store using Athena query using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "account_id = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "query_results= 'sagemaker-featurestore-workshop'\n",
    "prefix = 'sagemaker-feature-store'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize boto3 runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive the orders, products and customers feature group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "# Retreive FG row count\n",
    "%store -r customers_count\n",
    "%store -r products_count\n",
    "%store -r orders_count\n",
    "\n",
    "customers_fg = FeatureGroup(name=customers_feature_group_name, sagemaker_session=feature_store_session)  \n",
    "products_fg = FeatureGroup(name=products_feature_group_name, sagemaker_session=feature_store_session)\n",
    "orders_fg = FeatureGroup(name=orders_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Athena join query to combine the 3 features groups - `customers`, `products` & `orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_query = customers_fg.athena_query()\n",
    "customers_table = customers_query.table_name\n",
    "\n",
    "products_query = products_fg.athena_query()\n",
    "products_table = products_query.table_name\n",
    "\n",
    "orders_query = orders_fg.athena_query()\n",
    "orders_table = orders_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f'SELECT * FROM \"{customers_table}\", \"{products_table}\", \"{orders_table}\" ' \\\n",
    "               f'WHERE (\"{orders_table}\".\"customer_id\" = \"{customers_table}\".\"customer_id\") ' \\\n",
    "               f'AND (\"{orders_table}\".\"product_id\" = \"{products_table}\".\"product_id\")'\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = f's3://{default_bucket}/{query_results}/query_results/'\n",
    "print(f'Athena query output location: \\n{output_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if data is available in offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before extracting the data we need to check if the feature store was populated\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:    \n",
    "    customers_total_record_count = Utils.get_historical_record_count(customers_feature_group_name)\n",
    "    products_total_record_count = Utils.get_historical_record_count(products_feature_group_name)\n",
    "    orders_total_record_count = Utils.get_historical_record_count(orders_feature_group_name)\n",
    "    if customers_total_record_count >= customers_count and \\\n",
    "        products_total_record_count >= products_count and \\\n",
    "        orders_total_record_count >= orders_count:\n",
    "        logger.info('[Features are available in Offline Store!]')\n",
    "        offline_store_contents = orders_total_record_count\n",
    "    else:\n",
    "        logger.info('[Waiting for data in Offline Store ...]')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Athena query and load the output as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_query.run(query_string=query_string, output_location=output_location)\n",
    "orders_query.wait()\n",
    "joined_df = orders_query.as_dataframe()\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a feature set\n",
    "\n",
    "**Note:** This extracted feature set will be used for model training in `Module-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns which are not needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = joined_df.drop(['order_id', \n",
    "                           'customer_id', \n",
    "                           'product_id', \n",
    "                           'event_time', \n",
    "                           'write_time', \n",
    "                           'api_invocation_time', \n",
    "                           'is_deleted', \n",
    "                           'product_id.1', \n",
    "                           'event_time.1', \n",
    "                           'write_time.1', \n",
    "                           'api_invocation_time.1', \n",
    "                           'is_deleted.1', \n",
    "                           'customer_id.1', \n",
    "                           'purchase_amount',\n",
    "                           'event_time.2', \n",
    "                           'n_days_since_last_purchase',\n",
    "                           'write_time.2', \n",
    "                           'api_invocation_time.2', \n",
    "                           'is_deleted.2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write transformed features to local `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('.././data/train/transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy file from local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(default_bucket).Object(os.path.join(query_results, 'transformed.csv')).upload_file('.././data/train/transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional queries and validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browse the set of offline store files in the S3 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_s3_console_url = Utils.get_offline_store_url(customers_feature_group_name)\n",
    "products_s3_console_url = Utils.get_offline_store_url(products_feature_group_name)\n",
    "orders_s3_console_url = Utils.get_offline_store_url(orders_feature_group_name)\n",
    "\n",
    "logger.info('Review customers offline store partitioned data files here: '+customers_s3_console_url)\n",
    "logger.info('\\nReview products offline store partitioned data files here: '+products_s3_console_url)\n",
    "logger.info('\\nReview orders offline store partitioned data files here: '+orders_s3_console_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Glue tables that are used for Athena queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_glue_console_url = Utils.get_glue_table_url(customers_feature_group_name)\n",
    "products_glue_console_url = Utils.get_glue_table_url(products_feature_group_name)\n",
    "orders_glue_console_url = Utils.get_glue_table_url(orders_feature_group_name)\n",
    "\n",
    "logger.info('To see the customers Glue table that was created for you, go here: '+customers_glue_console_url)\n",
    "logger.info('\\nTo see the products Glue table that was created for you, go here: '+products_glue_console_url)\n",
    "logger.info('\\nTo see the orders Glue table that was created for you, go here: '+orders_glue_console_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine contents of a sample offline store Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look to the customers FG\n",
    "customers_sample_filename = Utils.download_sample_offline_file(customers_feature_group_name)\n",
    "logger.info('Downloaded sample Parquet file from offline store: '+customers_sample_filename+'\\n')\n",
    "\n",
    "customers_sample_df = pd.read_parquet(customers_sample_filename)\n",
    "customers_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the rows in an offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows from customers FG\n",
    "customers_total_record_count = Utils.get_historical_record_count(customers_feature_group_name)\n",
    "logger.info(f'Found {customers_total_record_count:,d} records in \"{customers_feature_group_name}\" feature group.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a random sample of offline store rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random rows from customers FG\n",
    "customers_sample_df = Utils.sample(customers_feature_group_name, n=5)\n",
    "customers_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
