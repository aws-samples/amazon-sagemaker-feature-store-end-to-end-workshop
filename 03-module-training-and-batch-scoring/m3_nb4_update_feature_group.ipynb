{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Update Feature Group (Optional notebook)\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "1. [Explore existing customer feature group and data](#explore-customer-fg)\n",
    "1. [Update customer feature group](#update-customer-fg)\n",
    "1. [Ingest data into customer feature group](#ingest-customer-fg)\n",
    "1. [Prepare training data set to retrain model](#model-training-data)\n",
    "1. [Retrain XG Boost model](#retrain-xg-boost)\n",
    "1. [Test model performance against test data](#real-time-inference)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will illustrate how to modify a feature group using boto3 API and then ingest data into modified feature group. We will cover the following aspects:\n",
    "\n",
    "* Look at existing data from customer feature group\n",
    "* Modify customer feature group to add \"has_kids\" feature and ingest sample data\n",
    "* Verify for a customer record that data has been ingested\n",
    "* Athena query for dataset extraction to prepare data set for retraining(programmatically using SageMaker SDK)\n",
    "* Retrain an XGBoost model similar to what we did in the notebook `m3_nb1_model_training.ipynb` in this module\n",
    "* Test by deploying the model and predicting against test data\n",
    "* Cleanup resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime, timezone, date\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sagemaker\n",
    "import importlib\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sagemaker.__version__ < '2.48.1':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker==2.48.1'])\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if boto3.__version__ < '1.24.23':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'boto3==1.24.23'])\n",
    "    importlib.reload(boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Pandas version: {pd.__version__}')\n",
    "logger.info(f'Using boto3 version: {boto3.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pretty_printer = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "logger.info(f'Default S3 bucket = {default_bucket}')\n",
    "prefix = 'sagemaker-feature-store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_runtime = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "s3 = boto_session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event_timestamp():\n",
    "    # naive datetime representing local time\n",
    "    naive_dt = datetime.now()\n",
    "    # take timezone into account\n",
    "    aware_dt = naive_dt.astimezone()\n",
    "    # time in UTC\n",
    "    utc_dt = aware_dt.astimezone(timezone.utc)\n",
    "    # transform to ISO-8601 format\n",
    "    event_time = utc_dt.isoformat(timespec='milliseconds')\n",
    "    event_time = event_time.replace('+00:00', 'Z')\n",
    "    return event_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore existing feature definition and the data set\n",
    "<a id='explore-customer-fg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve variables stored in previous notebooks for feature group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "logger.info(f'Customers FG: {customers_feature_group_name}')\n",
    "logger.info(f'Products FG: {products_feature_group_name}')\n",
    "logger.info(f'Orders FG: {orders_feature_group_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg = FeatureGroup(name=customers_feature_group_name, sagemaker_session=sagemaker_session)  \n",
    "products_fg = FeatureGroup(name=products_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "orders_fg = FeatureGroup(name=orders_feature_group_name, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify record exists in Customer Feature Group for a random customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id =  f'C{randint(1, 10000)}'\n",
    "logger.info(f'customer_id={customer_id}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_record = featurestore_runtime.get_record(FeatureGroupName=customers_feature_group_name, \n",
    "                                                        RecordIdentifierValueAsString=customer_id)\n",
    "feature_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_feature_group_result = sagemaker_runtime.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "pretty_printer.pprint(describe_feature_group_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update feature group and ingest data\n",
    "<a id='update-customer-fg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample product set that we have are spread out across different categories - baby products, candies, cleaning products etc. So let us assume that a customer *“having kids or not”* is defintely an indicator of them buying baby and kids products. Lets go ahead and modify the customer feature group to add this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call UpdateFeatureGroup with feature addition(s)\n",
    "sagemaker_runtime.update_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name,\n",
    "    FeatureAdditions=[\n",
    "        {\"FeatureName\": \"has_kids\", \"FeatureType\": \"Integral\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_feature_group_result = sagemaker_runtime.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "pretty_printer.pprint(describe_feature_group_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare \"has_kids\" feature data and ingest data again into customer feature group. \n",
    "<a id='ingest-customer-fg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the customer data from the csv and randomy generate 0 or 1 for \"has_kids\" feature and ingest into feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_csv('.././data/transformed/customers.csv')\n",
    "customers_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df['has_kids']=np.random.randint(0, 2, customers_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df=customers_df.drop(['event_time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_timestamps = [generate_event_timestamp() for _ in range(len(customers_df))]\n",
    "customers_df['event_time'] = event_timestamps\n",
    "customers_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "customers_fg.ingest(data_frame=customers_df, max_processes=16, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify this new data for the randomly generated customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_record_result = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=customers_feature_group_name,\n",
    "    RecordIdentifierValueAsString=customer_id\n",
    ")\n",
    "pretty_printer.pprint(get_record_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run Athena query to verify offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_query = customers_fg.athena_query()\n",
    "customers_table = customers_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = f's3://{default_bucket}/{prefix}/query_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_query.run(query_string=query_string,output_location=output_location)\n",
    "customers_query.wait()\n",
    "athena_df = customers_query.as_dataframe()\n",
    "athena_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the above step, it is very easy now to modify an existing feature group, add new features, and ingest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_csv('.././data/transformed/customers_has_kids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional steps\n",
    "From here on in this notebook, we use the data that has the new feature \"has_kids\" and train the model again with the data, deploy the model and test it against sample data. The intention is not to prove that model performance improves (mind you this is sample data!) but to show a real life use case where modified feature groups can be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify offline store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query in Athena console\n",
    "\n",
    "If it is for the first time we are launching Athena in AWS console we need to click on `Get Started` button and then before we run the first query we need to set up a query results location in Amazon S3. \n",
    "\n",
    "After setting the query results location, on the left panel we need to select the `AwsDataCatalog` as Data source and the `sagemaker_featurestore` as Database.\n",
    "\n",
    "We can run now run a query for the offline feature store data in Athena. To select the entries from the orders feature group we use the following SQL query. You will need to replace the orders table name with the corresponded value from your environment.\n",
    "\n",
    "```sql\n",
    "select * from \"<customers-table>\"\n",
    "limit 100\n",
    "```\n",
    "\n",
    "![Customers offline data](../images/m3_nb4_athena_query.png \"Customers Offline Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model training dataset\n",
    "<a id='model-training-data' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train, test and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_query = products_fg.athena_query()\n",
    "products_table = products_query.table_name\n",
    "\n",
    "orders_query = orders_fg.athena_query()\n",
    "orders_table = orders_query.table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare training, validation and test data, we run an Athena query against offline feature store and get records for which \"has_kids\" has been populated. Why do we do this? Because offline feature store has historical records, we want only the latest ingested data that has \"has_kids\" populated for retraining our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f'SELECT * FROM \"{customers_table}\", \"{products_table}\", \"{orders_table}\" ' \\\n",
    "               f'WHERE (\"{orders_table}\".\"customer_id\" = \"{customers_table}\".\"customer_id\") ' \\\n",
    "               f'AND (\"{orders_table}\".\"product_id\" = \"{products_table}\".\"product_id\")' \\\n",
    "               f'AND (\"{customers_table}\".\"has_kids\" is not null)'\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_query.run(query_string=query_string, output_location=output_location)\n",
    "orders_query.wait()\n",
    "joined_df = orders_query.as_dataframe()\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = joined_df.drop(['order_id', \n",
    "                           'customer_id', \n",
    "                           'product_id', \n",
    "                           'event_time', \n",
    "                           'write_time', \n",
    "                           'api_invocation_time', \n",
    "                           'is_deleted', \n",
    "                           'product_id.1', \n",
    "                           'event_time.1', \n",
    "                           'write_time.1', \n",
    "                           'api_invocation_time.1', \n",
    "                           'is_deleted.1', \n",
    "                           'customer_id.1', \n",
    "                           'purchase_amount',\n",
    "                           'event_time.2', \n",
    "                           'n_days_since_last_purchase',\n",
    "                           'write_time.2', \n",
    "                           'api_invocation_time.2', \n",
    "                           'is_deleted.2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = model_df.pop('is_reordered')\n",
    "model_df.insert(0, 'is_reordered', first_column)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('.././data/train/transformed_has_kids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the XGBoost model with the update feature group\n",
    "<a id='retrain-xg-boost' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the model again with this new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = np.split(model_df.sample(frac=1, random_state=123), [int(.7*len(model_df)), int(.9*len(model_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/retrain/train.csv', index=False)\n",
    "validation_df.to_csv('../data/retrain/validation.csv', index=False)\n",
    "test_df.to_csv('../data/retrain/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'retrain/train.csv')).upload_file('.././data/retrain/train.csv')\n",
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'retrain/validation.csv')).upload_file('.././data/retrain/validation.csv')\n",
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'retrain/test.csv')).upload_file('.././data/retrain/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_location = 's3://{}/{}/retrain/'.format(default_bucket, prefix)\n",
    "validation_set_location = 's3://{}/{}/retrain/'.format(default_bucket, prefix)\n",
    "test_set_location = 's3://{}/{}/retrain/'.format(default_bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_pointer = TrainingInput(s3_data=train_set_location, content_type='csv')\n",
    "validation_set_pointer = TrainingInput(s3_data=validation_set_location, content_type='csv')\n",
    "test_set_pointer = TrainingInput(s3_data=test_set_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(region=boto_session.region_name, \n",
    "                                              framework='xgboost', \n",
    "                                              version='1.0-1', \n",
    "                                              image_scope='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(image_uri=container_uri,\n",
    "                                    role=role, \n",
    "                                    instance_count=2, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/model-artifacts'.format(default_bucket, prefix),\n",
    "                                    sagemaker_session=sagemaker_session,\n",
    "                                    base_job_name='reorder-classifier')\n",
    "\n",
    "xgb.set_hyperparameters(objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': train_set_pointer, 'validation': validation_set_pointer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=2,\n",
    "                           instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time inference using the deployed endpoint\n",
    "<a id='real-time-inference' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a record from test data and test the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_serializer = CSVSerializer()\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "predictor = Predictor(endpoint_name=endpoint_name, \n",
    "                      serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('.././data/retrain/test.csv')\n",
    "record = test_df.sample(1)\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = record.values[0]\n",
    "payload = X[1:]\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predicted_class_prob = predictor.predict(payload).decode('utf-8')\n",
    "if float(predicted_class_prob) < 0.5:\n",
    "    logger.info('Prediction (y) = Will not reorder')\n",
    "else:\n",
    "    logger.info('Prediction (y) = Will reorder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how features can be added to feature groups, it is time to delete unwated resources to not inciur charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_feature_group_result = sagemaker_runtime.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "pretty_printer.pprint(describe_feature_group_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the S3 artifacts for the offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = describe_feature_group_result['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "full_prefix = '/'.join(s3_uri.split('/')[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(default_bucket)\n",
    "offline_objects = bucket.objects.filter(Prefix=full_prefix)\n",
    "offline_objects.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_feature_group.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "model_name = response['ProductionVariants'][0]['ModelName']\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_model(ModelName=model_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
