{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Batch Scoring using a pre-trained XGBoost model\n",
    "**This notebook uses the feature store to prepare test dataset for batch scoring and then use the XGBoost model trained in the model training notebook**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Prepare test data](#Prepare-test-data)\n",
    "1. [Batch Transform](#Batch-Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "After the model is trained, if the goal is to generate predictions on a large dataset where minimizing latency isn't a concern, then SageMaker batch transform is the solution. Functionally, batch transform uses the same mechanics as real-time hosting to generate predictions. It requires a web server that takes in HTTP POST requests a single observation, or mini-batch, at a time. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes.\n",
    "\n",
    "In this example, we will walk through the steps to prepare the batch test dataset from feature store using athena CTAS query and perform batch transform with the test data available on Amazon S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from time import sleep\n",
    "from urllib.parse import urlparse\n",
    "from io import StringIO\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utilities import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"__name__\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = get_execution_role()\n",
    "logger.info(f\"Role = {sagemaker_execution_role}\")\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = session.client(service_name=\"sagemaker\")\n",
    "\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker-featurestore-workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def list_s3_files(s3uri):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket = s3.Bucket(parsed_url.netloc)\n",
    "    prefix = parsed_url.path[1:]\n",
    "    return [\n",
    "        dict(bucket_name=k.bucket_name, key=k.key)\n",
    "        for k in bucket.objects.filter(Prefix=prefix)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data for batch transform \n",
    "<!-- job using processing job with *AthenaDatasetDefinition* -->\n",
    "We create the test dataset that we will use in our batch transform job using [*Athena CREATE TABLE AS SELECT (CTAS) query*](https://docs.aws.amazon.com/athena/latest/ug/ctas.html). A CTAS query creates a new table in Athena from the results of a SELECT statement from another query. Athena stores data files created by the CTAS statement in a specified location in Amazon S3.\n",
    "\n",
    "We follow the steps below to prepare the test dataset for batch transform job:\n",
    "1. firstly generates the list of feature names that we would like to read from the offline feature store by providing the feature group names as a list and an exclude feature list to the *generate_fsets* function. \n",
    "2. Construct an Athena SELECT query to get the expected test data from the target feature groups and then construct a CTAS query based on the first query to transform query results into *Parquet* format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate the list of features needed from feature store.\n",
    "\n",
    "We use boto3 sagemaker_client to perform `DescribeFeatureGroup` action to describe a FeatureGroup. The response includes information on the creation time, FeatureGroup name, the unique identifier for each FeatureGroup, and more, for more details of the response syntax, please refer to [document here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeFeatureGroup.html#API_DescribeFeatureGroup_ResponseSyntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "\n",
    "customers_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "products_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=products_feature_group_name\n",
    ")\n",
    "orders_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=orders_feature_group_name\n",
    ")\n",
    "\n",
    "database_name = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Database\"]\n",
    "catalog = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Catalog\"]\n",
    "\n",
    "customers_table = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "products_table = products_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "orders_table = orders_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_fsets = [\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "    \"event_time\",\n",
    "    \"purchase_amount\",\n",
    "    \"n_days_since_last_purchase\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(fg_list, exclude_fsets=None):\n",
    "    _fg_lst = []\n",
    "    for _fg in fg_list:\n",
    "        _fg_tmp = pd.DataFrame(\n",
    "            Utils.describe_feature_group(_fg[\"FeatureGroupName\"])[\"FeatureDefinitions\"]\n",
    "        )\n",
    "        if exclude_fsets:\n",
    "            _fg_tmp = _fg_tmp[~_fg_tmp.FeatureName.isin(exclude_fsets)]\n",
    "\n",
    "        _fg_lst.append(_fg_tmp)\n",
    "    return pd.concat(_fg_lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsets_df = generate_fsets([orders_fg, customers_fg, products_fg], exclude_fsets)\n",
    "features_names = fsets_df.FeatureName.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Athena CTAS table query to generate test set for batch job\n",
    "\n",
    "\n",
    "\n",
    "We start by create an Athena query to get the test data from feature store. We can use [PyAthena](https://pypi.org/project/pyathena/) (a library that uses [Athena's REST API](https://docs.aws.amazon.com/athena/latest/APIReference/Welcome.html) to connect to Athena and fetech query results) or [boto3 Athena](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/athena.html) (a low-level client representing Amazon Athena) to run queries that derive the feature sets from the Offline Feature Store. In this example, we can use CTAS statements to create new tables from existing tables on a subset of data, or a subset of columns. CTAS statements help reduce cost and improve performance by allowing users to run queries on smaller tables constructed from larger tables. When creating new tables using CTAS, you can include a WITH statement to define table-specific parameters, such as file format, compression, and partition columns. For more information, please refer to this [blog](https://aws.amazon.com/blogs/big-data/using-ctas-statements-with-amazon-athena-to-reduce-cost-and-improve-performance/). Note that the first column will be the unique identifier of customer id and the second column is the target value. Note that the query should only take the latest version of any given record that has multiple write times for the same event_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_columns_string = \",\\n    \".join(f'\"{c}\"' for c in features_names)\n",
    "\n",
    "customer_uid = customers_fg[\"RecordIdentifierFeatureName\"]\n",
    "product_uid = products_fg[\"RecordIdentifierFeatureName\"]\n",
    "order_uid = orders_fg[\"RecordIdentifierFeatureName\"]\n",
    "\n",
    "customer_et = customers_fg[\"EventTimeFeatureName\"]\n",
    "product_et = products_fg[\"EventTimeFeatureName\"]\n",
    "order_et = orders_fg[\"EventTimeFeatureName\"]\n",
    "\n",
    "destination_s3_path = f's3://{default_bucket}/{prefix}/athena/data/sagemaker-batch-{time.strftime(\"%Y-%m-%d-%H%M%S\")}/data'\n",
    "\n",
    "temp_database_name = \"sagemaker_processing\"\n",
    "table_name = f\"sagemaker_tmp_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "query_string = f\"\"\"WITH customer_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{customer_uid}\"\n",
    "            ORDER BY \"{customer_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{customers_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "product_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{product_uid}\"\n",
    "            ORDER BY \"{product_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{products_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "order_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{order_uid}\"\n",
    "            ORDER BY \"{order_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{orders_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    ")\n",
    "\n",
    "SELECT DISTINCT\n",
    "    \"{order_uid}\",\n",
    "    {batch_transform_columns_string}\n",
    "FROM customer_table,\n",
    "    product_table,\n",
    "    order_table\n",
    "WHERE order_table.\"customer_id\" = customer_table.\"customer_id\"\n",
    "    AND order_table.\"product_id\" = product_table.\"product_id\"\n",
    "    AND customer_table.\"rank\" = 1\n",
    "    AND product_table.\"rank\" = 1\n",
    "    AND order_table.\"rank\" = 1\n",
    "\"\"\"\n",
    "\n",
    "# create a temporary table with external_localtion pointing to the s3 location as save the query results as PARQUET\n",
    "CTAS_query = f\"\"\"CREATE TABLE {catalog}.{temp_database_name}.{table_name}\n",
    "WITH (external_location='{destination_s3_path}', format='PARQUET') \n",
    "AS {query_string}\n",
    "\"\"\"\n",
    "print(CTAS_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client(\"athena\")\n",
    "glue = boto3.client(\"glue\")\n",
    "\n",
    "tmp_uri = f\"s3://{default_bucket}/{prefix}/offline-store/query_results/\"\n",
    "logger.info(f\"Running query on  database: {database_name}\")\n",
    "query_execution = athena.start_query_execution(\n",
    "    QueryString=CTAS_query,\n",
    "    QueryExecutionContext={\"Database\": database_name},\n",
    "    ResultConfiguration={\"OutputLocation\": tmp_uri},\n",
    ")\n",
    "# wait for the Athena query to complete\n",
    "query_execution_id = query_execution[\"QueryExecutionId\"]\n",
    "\n",
    "while True:\n",
    "    query_response = athena.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "    query_state = query_response[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "    if query_state == \"SUCCEEDED\":\n",
    "        # !aws s3 ls $destination_s3_path/\n",
    "        print(\"\", end=\"\\r\")\n",
    "        [print(k[\"key\"]) for k in list_s3_files(destination_s3_path)]\n",
    "        break\n",
    "    if query_state == \"FAILED\":\n",
    "        logger.info(json.dumps(query_response, indent=2, default=str))\n",
    "        break\n",
    "    print(\".\", end=\"\")\n",
    "    sleep(0.2)\n",
    "try:\n",
    "    glue.delete_table(DatabaseName=temp_database_name, Name=table_name)\n",
    "    logger.info(\"Temporary table removed from Glue Catalog\")\n",
    "except:\n",
    "    logger.error(\"Failed to delete the temporary table in Glue Catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Batch Transform, we introduced 3 new attributes - __input_filter__, __join_source__ and __output_filter__. In the below cell, we use the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to kick-off several Batch Transform jobs using different configurations of these 3 new attributes. Please refer to [this page](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model based on the pre-trained model artifacts on S3\n",
    "Let's first create a model based on the training job from the previous notebook. We can use `describe_training_job` boto3 api call to get the model data uri. We use the [*XGBoostModel*](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/xgboost/model.py#L66) class (Framework Model) from the SageMaker SDK to create the model for batch transform. Note that you need to use the same framework version as the training job and also provide [an entry point script ](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#write-an-inference-script) that implements (at least) the `model_fn` function that calls the loaded model to get a prediction. In this example, we implemented the `input_fn` to handle the Parquet input format (the content type is \"application/x-parquet\") and transform the data to pandas dataframe. In the `predict_fn`, we filter out the ID column and target column and convert the feature columns from dataframe to *DMatrix* data type for prediction. We also associate the input with the prediction results by appending the prediction results as an additional column to the input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_jobName\n",
    "\n",
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "\n",
    "training_job_info = sagemaker_client.describe_training_job(\n",
    "    TrainingJobName=training_jobName\n",
    ")\n",
    "xgb_model_data = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    source_dir=\"./code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    model_data=xgb_model_data,\n",
    "    role=sagemaker_execution_role,\n",
    "    name=name_from_base(\"fs-workshop-xgboost-model\"),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used CTAS query to create a new table in Parquet format from the source tables, as Parquet is a widely used data storage format which is efficient and performant in both storage and processing. You can use the format property to specify ORC, AVRO, JSON, or TEXTFILE as the storage format for the new table. However, by default the output data is compressed which means we cannot save the data directly as CSV format (but rather the compressed CSV format with 'gz' extension). None of these output formats (with compression) is supported by batch transform directly, therefore, we need to either use a processing job to convert the data format to a supported one or treat each file as one record so the batch transform service doesn't need to read the file and slipt the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, although batch transform support [association of prediction results with input](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) and can filter out columns in the input data when the input data is in the supported formats. The PARQUET data type is not supported by default (the supported input data formats are JSON- or CSV-), therefore, we will set the [batch strategy](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#API_CreateTransformJob_RequestParameters) to `SingleRecord` which will read each parquet file as a whole and send the payload to the model for inference. The `max_concurrent_transforms` specifies the maximum number of parallel requests that can be sent to each instance in a transform job. Please note that as each file is used as one transform request, the default value for `MaxPayloadInMB` is 6MB. If the parquet file size for your test data is more than 6MB, you need to change the `max_payload` to be equal or greater than the maximum file size of the parquet files in S3. The `accept` parameter specifies the format of the batch transform output; where the `content_type` defines the MIME type of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = f\"s3://{default_bucket}/{prefix}/batch_output/{xgb_model.name}\"\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(\n",
    "    strategy=\"SingleRecord\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_concurrent_transforms=4,\n",
    "    accept=\"text/csv\",\n",
    "    output_path=output_path,\n",
    ")\n",
    "xgb_transformer.transform(\n",
    "    destination_s3_path,\n",
    "    content_type=\"application/x-parquet\",  \n",
    "# if you don't want to wait for the batch job to finish, uncomment below line and check the batch transform job status on the SageMaker console \n",
    "#     wait=False,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. We can list all the generated files. They are in CSV format, to get PARQUET (or any other supported format) it is possible to add an `output_fn` to `inference.py` to convert the inference result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_list = list_s3_files(xgb_transformer.output_path)\n",
    "[print(k[\"key\"]) for k in output_file_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", 1200):\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            [k[\"key\"].split(\"/\")[-1] for k in output_file_list], columns=[\"File name\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read read and combine the output into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_obj = s3.Object(**output_file_list[0])\n",
    "body = s3_obj.get()['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "pd.read_csv(\n",
    "    StringIO(csv_string),\n",
    "    header=None,\n",
    "    names=features_names + [\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
