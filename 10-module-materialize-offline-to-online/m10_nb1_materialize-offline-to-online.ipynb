{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialize Features from the Offline to Online Store\n",
    "\n",
    "In this example, we demonstrate how customers can use the [Feature Store Spark Connector](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html) to ingest features directly to the offline store, and incrementally materialize the latest features to the online store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Group\n",
    "\n",
    "First, create a feature group with online and offline stores configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region_name = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "feature_group_name = 'feature-store-offline-to-online-example'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highly recommend storing offline features using the Apache Iceberg table format. If you need to use the Glue table format, please update the variable below to `'Glue'`. \n",
    "\n",
    "For more information on offline store formats, please refer to the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-offline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_format = 'Iceberg' # or 'Glue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This example only works with the Glue table format.\n",
    "\n",
    "create_feature_group_output = sm_client.create_feature_group(\n",
    "    FeatureGroupName=feature_group_name,\n",
    "    RecordIdentifierFeatureName='RecordIdentifier',\n",
    "    EventTimeFeatureName='EventTime',\n",
    "    OnlineStoreConfig={\n",
    "        'EnableOnlineStore': True\n",
    "    },\n",
    "    OfflineStoreConfig={\n",
    "        'S3StorageConfig': {\n",
    "            'S3Uri': f's3://{default_bucket}'\n",
    "        },\n",
    "        'TableFormat': table_format\n",
    "    },\n",
    "    FeatureDefinitions=[\n",
    "        {\n",
    "            'FeatureName': 'RecordIdentifier',\n",
    "            'FeatureType': 'Integral'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'Measure',\n",
    "            'FeatureType': 'Fractional'\n",
    "        },\n",
    "        {\n",
    "            'FeatureName': 'EventTime',\n",
    "            'FeatureType': 'String'\n",
    "        }\n",
    "    ],\n",
    "    RoleArn=role\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data to the Offline Store\n",
    "\n",
    "We will create a [SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) which uses the Feature Store Spark Connector to ingests a set of features directly into the offline store.\n",
    "\n",
    "To use the Feature Store Spark Connector in a Processing Job, we recommend extending the prebuilt SageMaker Spark Processing container as shown in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html#:~:text=Installation%20on%20a%20Amazon%20SageMaker%20Processing%20Job\n",
    "). For this example, we have included the Python interface to the Spark Connector under `feature_store_pyspark`. We will download the JAR file which encapsulates the Spark Connector functionality and submit all of these assets when run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_version = '3.1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the correct JAR, we will use `pip` to install the specific connector library based on the Spark version (`MAJOR.MINOR`) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker-feature-store-pyspark-{spark_version}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the command line tool to fetch the location of the JAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = !feature-store-pyspark-dependency-jars\n",
    "jar_path = jar_path[0]\n",
    "jar_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a processing job using `scripts/ingest_offline.py` and include the necessary Python modules and JAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    max_runtime_in_seconds=1200,\n",
    "    framework_version=spark_version,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app='./scripts/ingest_offline.py',\n",
    "    arguments=[\n",
    "        '--feature_group_name',\n",
    "        feature_group_name,\n",
    "        '--region_name',\n",
    "        region_name\n",
    "    ],\n",
    "    logs=False,\n",
    "    submit_jars=[jar_path],\n",
    "    submit_py_files=[\n",
    "        './feature_store_pyspark/feature_store_manager.py',\n",
    "        './feature_store_pyspark/wrapper.py'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialize Latest Features to Online Store\n",
    "\n",
    "Now that our features are ingested to offline store, we can materialize the latest features (for each record identifier) to the online store. To do this, we we will run another Spark Processing Job using `scripts/materialize.py`. Since the task may need to run on a regular cadence, we can add the processing job to a SageMaker Pipeline. This pipeline can then be scheduled with [Amazon EventBridge](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "pipeline_name = feature_group_name\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    framework_version=spark_version\n",
    ")\n",
    "\n",
    "processor_args = spark_processor.run(\n",
    "    submit_app='./scripts/materialize.py',\n",
    "    logs=False,\n",
    "    arguments = [\n",
    "        '--table_format',\n",
    "        table_format,\n",
    "        '--feature_group_name',\n",
    "        feature_group_name,\n",
    "        '--region_name',\n",
    "        region_name\n",
    "    ],\n",
    "    submit_jars=[\n",
    "        jar_path\n",
    "    ],\n",
    "    submit_py_files=[\n",
    "        './feature_store_pyspark/feature_store_manager.py',\n",
    "        './feature_store_pyspark/wrapper.py'\n",
    "    ]\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name='MaterializeToOnlineStore', step_args=processor_args)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[step_process],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "execution.wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the latest features are available in the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_client = boto3.client('sagemaker-featurestore-runtime')\n",
    "fs_client.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName': feature_group_name,\n",
    "            'RecordIdentifiersValueAsString': ['1', '2', '3']\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
